Processing dataset: HEW10_tax_massProcessing with K = 10, strategy: random
[DivideNet] Using directed logic. Connectivity check: 1
[DivideNet] Test links accepted: 173 / 173 (100.0%)
[DivideNet] Attempts made: 226 | Failed attempts: 53
    "Experiment "    "1"    " (strategy: "    "random"    ") — Running WLNM..."

Warning: [sample_neg] Not enough negatives. Reducing a...
> In <a href="matlab:matlab.lang.internal.introspective.errorDocCallback('sample_neg', '/Users/jorge/Documents/qmul-phd-framework/src/matlab/sample_neg.m', 82)" style="font-weight:bold">sample_neg</a> (<a href="matlab: opentoline('/Users/jorge/Documents/qmul-phd-framework/src/matlab/sample_neg.m',82,0)">line 82</a>)
In <a href="matlab:matlab.lang.internal.introspective.errorDocCallback('WLNM', '/Users/jorge/Documents/qmul-phd-framework/src/matlab/WLNM.m', 33)" style="font-weight:bold">WLNM</a> (<a href="matlab: opentoline('/Users/jorge/Documents/qmul-phd-framework/src/matlab/WLNM.m',33,0)">line 33</a>)
In <a href="matlab:matlab.lang.internal.introspective.errorDocCallback('Main>processExperiment', '/Users/jorge/Documents/qmul-phd-framework/src/matlab/Main.m', 220)" style="font-weight:bold">Main>processExperiment</a> (<a href="matlab: opentoline('/Users/jorge/Documents/qmul-phd-framework/src/matlab/Main.m',220,0)">line 220</a>)
Warning: [sample_neg] Using fallback: unfiltered negatives with a = 1.
> In <a href="matlab:matlab.lang.internal.introspective.errorDocCallback('sample_neg', '/Users/jorge/Documents/qmul-phd-framework/src/matlab/sample_neg.m', 87)" style="font-weight:bold">sample_neg</a> (<a href="matlab: opentoline('/Users/jorge/Documents/qmul-phd-framework/src/matlab/sample_neg.m',87,0)">line 87</a>)
In <a href="matlab:matlab.lang.internal.introspective.errorDocCallback('WLNM', '/Users/jorge/Documents/qmul-phd-framework/src/matlab/WLNM.m', 33)" style="font-weight:bold">WLNM</a> (<a href="matlab: opentoline('/Users/jorge/Documents/qmul-phd-framework/src/matlab/WLNM.m',33,0)">line 33</a>)
In <a href="matlab:matlab.lang.internal.introspective.errorDocCallback('Main>processExperiment', '/Users/jorge/Documents/qmul-phd-framework/src/matlab/Main.m', 220)" style="font-weight:bold">Main>processExperiment</a> (<a href="matlab: opentoline('/Users/jorge/Documents/qmul-phd-framework/src/matlab/Main.m',220,0)">line 220</a>)
[sample_neg] Final link counts (use_role_filter = 1):
    Train Positive: 716
    Train Negative: 716
    Test  Positive: 173
    Test  Negative: 173
[WLNM] Skipping DegreeCompute: non-degree-based strategy selected.
Encoding 1432 subgraphs (K = 10)...
    "Experiment "    "2"    " (strategy: "    "random"    ") — Running WLNM..."

Warning: [sample_neg] Not enough negatives. Reducing a...
> In <a href="matlab:matlab.lang.internal.introspective.errorDocCallback('sample_neg', '/Users/jorge/Documents/qmul-phd-framework/src/matlab/sample_neg.m', 82)" style="font-weight:bold">sample_neg</a> (<a href="matlab: opentoline('/Users/jorge/Documents/qmul-phd-framework/src/matlab/sample_neg.m',82,0)">line 82</a>)
In <a href="matlab:matlab.lang.internal.introspective.errorDocCallback('WLNM', '/Users/jorge/Documents/qmul-phd-framework/src/matlab/WLNM.m', 33)" style="font-weight:bold">WLNM</a> (<a href="matlab: opentoline('/Users/jorge/Documents/qmul-phd-framework/src/matlab/WLNM.m',33,0)">line 33</a>)
In <a href="matlab:matlab.lang.internal.introspective.errorDocCallback('Main>processExperiment', '/Users/jorge/Documents/qmul-phd-framework/src/matlab/Main.m', 220)" style="font-weight:bold">Main>processExperiment</a> (<a href="matlab: opentoline('/Users/jorge/Documents/qmul-phd-framework/src/matlab/Main.m',220,0)">line 220</a>)
Warning: [sample_neg] Using fallback: unfiltered negatives with a = 1.
> In <a href="matlab:matlab.lang.internal.introspective.errorDocCallback('sample_neg', '/Users/jorge/Documents/qmul-phd-framework/src/matlab/sample_neg.m', 87)" style="font-weight:bold">sample_neg</a> (<a href="matlab: opentoline('/Users/jorge/Documents/qmul-phd-framework/src/matlab/sample_neg.m',87,0)">line 87</a>)
In <a href="matlab:matlab.lang.internal.introspective.errorDocCallback('WLNM', '/Users/jorge/Documents/qmul-phd-framework/src/matlab/WLNM.m', 33)" style="font-weight:bold">WLNM</a> (<a href="matlab: opentoline('/Users/jorge/Documents/qmul-phd-framework/src/matlab/WLNM.m',33,0)">line 33</a>)
In <a href="matlab:matlab.lang.internal.introspective.errorDocCallback('Main>processExperiment', '/Users/jorge/Documents/qmul-phd-framework/src/matlab/Main.m', 220)" style="font-weight:bold">Main>processExperiment</a> (<a href="matlab: opentoline('/Users/jorge/Documents/qmul-phd-framework/src/matlab/Main.m',220,0)">line 220</a>)
[sample_neg] Final link counts (use_role_filter = 1):
    Train Positive: 716
    Train Negative: 716
    Test  Positive: 173
    Test  Negative: 173
[WLNM] Skipping DegreeCompute: non-degree-based strategy selected.
Encoding 1432 subgraphs (K = 10)...
    "Experiment "    "8"    " (strategy: "    "random"    ") — Running WLNM..."

Warning: [sample_neg] Not enough negatives. Reducing a...
> In <a href="matlab:matlab.lang.internal.introspective.errorDocCallback('sample_neg', '/Users/jorge/Documents/qmul-phd-framework/src/matlab/sample_neg.m', 82)" style="font-weight:bold">sample_neg</a> (<a href="matlab: opentoline('/Users/jorge/Documents/qmul-phd-framework/src/matlab/sample_neg.m',82,0)">line 82</a>)
In <a href="matlab:matlab.lang.internal.introspective.errorDocCallback('WLNM', '/Users/jorge/Documents/qmul-phd-framework/src/matlab/WLNM.m', 33)" style="font-weight:bold">WLNM</a> (<a href="matlab: opentoline('/Users/jorge/Documents/qmul-phd-framework/src/matlab/WLNM.m',33,0)">line 33</a>)
In <a href="matlab:matlab.lang.internal.introspective.errorDocCallback('Main>processExperiment', '/Users/jorge/Documents/qmul-phd-framework/src/matlab/Main.m', 220)" style="font-weight:bold">Main>processExperiment</a> (<a href="matlab: opentoline('/Users/jorge/Documents/qmul-phd-framework/src/matlab/Main.m',220,0)">line 220</a>)
Warning: [sample_neg] Using fallback: unfiltered negatives with a = 1.
> In <a href="matlab:matlab.lang.internal.introspective.errorDocCallback('sample_neg', '/Users/jorge/Documents/qmul-phd-framework/src/matlab/sample_neg.m', 87)" style="font-weight:bold">sample_neg</a> (<a href="matlab: opentoline('/Users/jorge/Documents/qmul-phd-framework/src/matlab/sample_neg.m',87,0)">line 87</a>)
In <a href="matlab:matlab.lang.internal.introspective.errorDocCallback('WLNM', '/Users/jorge/Documents/qmul-phd-framework/src/matlab/WLNM.m', 33)" style="font-weight:bold">WLNM</a> (<a href="matlab: opentoline('/Users/jorge/Documents/qmul-phd-framework/src/matlab/WLNM.m',33,0)">line 33</a>)
In <a href="matlab:matlab.lang.internal.introspective.errorDocCallback('Main>processExperiment', '/Users/jorge/Documents/qmul-phd-framework/src/matlab/Main.m', 220)" style="font-weight:bold">Main>processExperiment</a> (<a href="matlab: opentoline('/Users/jorge/Documents/qmul-phd-framework/src/matlab/Main.m',220,0)">line 220</a>)
[sample_neg] Final link counts (use_role_filter = 1):
    Train Positive: 716
    Train Negative: 716
    Test  Positive: 173
    Test  Negative: 173
[WLNM] Skipping DegreeCompute: non-degree-based strategy selected.
Encoding 1432 subgraphs (K = 10)...
    "Experiment "    "4"    " (strategy: "    "random"    ") — Running WLNM..."

Warning: [sample_neg] Not enough negatives. Reducing a...
> In <a href="matlab:matlab.lang.internal.introspective.errorDocCallback('sample_neg', '/Users/jorge/Documents/qmul-phd-framework/src/matlab/sample_neg.m', 82)" style="font-weight:bold">sample_neg</a> (<a href="matlab: opentoline('/Users/jorge/Documents/qmul-phd-framework/src/matlab/sample_neg.m',82,0)">line 82</a>)
In <a href="matlab:matlab.lang.internal.introspective.errorDocCallback('WLNM', '/Users/jorge/Documents/qmul-phd-framework/src/matlab/WLNM.m', 33)" style="font-weight:bold">WLNM</a> (<a href="matlab: opentoline('/Users/jorge/Documents/qmul-phd-framework/src/matlab/WLNM.m',33,0)">line 33</a>)
In <a href="matlab:matlab.lang.internal.introspective.errorDocCallback('Main>processExperiment', '/Users/jorge/Documents/qmul-phd-framework/src/matlab/Main.m', 220)" style="font-weight:bold">Main>processExperiment</a> (<a href="matlab: opentoline('/Users/jorge/Documents/qmul-phd-framework/src/matlab/Main.m',220,0)">line 220</a>)
Warning: [sample_neg] Using fallback: unfiltered negatives with a = 1.
> In <a href="matlab:matlab.lang.internal.introspective.errorDocCallback('sample_neg', '/Users/jorge/Documents/qmul-phd-framework/src/matlab/sample_neg.m', 87)" style="font-weight:bold">sample_neg</a> (<a href="matlab: opentoline('/Users/jorge/Documents/qmul-phd-framework/src/matlab/sample_neg.m',87,0)">line 87</a>)
In <a href="matlab:matlab.lang.internal.introspective.errorDocCallback('WLNM', '/Users/jorge/Documents/qmul-phd-framework/src/matlab/WLNM.m', 33)" style="font-weight:bold">WLNM</a> (<a href="matlab: opentoline('/Users/jorge/Documents/qmul-phd-framework/src/matlab/WLNM.m',33,0)">line 33</a>)
In <a href="matlab:matlab.lang.internal.introspective.errorDocCallback('Main>processExperiment', '/Users/jorge/Documents/qmul-phd-framework/src/matlab/Main.m', 220)" style="font-weight:bold">Main>processExperiment</a> (<a href="matlab: opentoline('/Users/jorge/Documents/qmul-phd-framework/src/matlab/Main.m',220,0)">line 220</a>)
[sample_neg] Final link counts (use_role_filter = 1):
    Train Positive: 716
    Train Negative: 716
    Test  Positive: 173
    Test  Negative: 173
[WLNM] Skipping DegreeCompute: non-degree-based strategy selected.
Encoding 1432 subgraphs (K = 10)...
    "Experiment "    "3"    " (strategy: "    "random"    ") — Running WLNM..."

Warning: [sample_neg] Not enough negatives. Reducing a...
> In <a href="matlab:matlab.lang.internal.introspective.errorDocCallback('sample_neg', '/Users/jorge/Documents/qmul-phd-framework/src/matlab/sample_neg.m', 82)" style="font-weight:bold">sample_neg</a> (<a href="matlab: opentoline('/Users/jorge/Documents/qmul-phd-framework/src/matlab/sample_neg.m',82,0)">line 82</a>)
In <a href="matlab:matlab.lang.internal.introspective.errorDocCallback('WLNM', '/Users/jorge/Documents/qmul-phd-framework/src/matlab/WLNM.m', 33)" style="font-weight:bold">WLNM</a> (<a href="matlab: opentoline('/Users/jorge/Documents/qmul-phd-framework/src/matlab/WLNM.m',33,0)">line 33</a>)
In <a href="matlab:matlab.lang.internal.introspective.errorDocCallback('Main>processExperiment', '/Users/jorge/Documents/qmul-phd-framework/src/matlab/Main.m', 220)" style="font-weight:bold">Main>processExperiment</a> (<a href="matlab: opentoline('/Users/jorge/Documents/qmul-phd-framework/src/matlab/Main.m',220,0)">line 220</a>)
Warning: [sample_neg] Using fallback: unfiltered negatives with a = 1.
> In <a href="matlab:matlab.lang.internal.introspective.errorDocCallback('sample_neg', '/Users/jorge/Documents/qmul-phd-framework/src/matlab/sample_neg.m', 87)" style="font-weight:bold">sample_neg</a> (<a href="matlab: opentoline('/Users/jorge/Documents/qmul-phd-framework/src/matlab/sample_neg.m',87,0)">line 87</a>)
In <a href="matlab:matlab.lang.internal.introspective.errorDocCallback('WLNM', '/Users/jorge/Documents/qmul-phd-framework/src/matlab/WLNM.m', 33)" style="font-weight:bold">WLNM</a> (<a href="matlab: opentoline('/Users/jorge/Documents/qmul-phd-framework/src/matlab/WLNM.m',33,0)">line 33</a>)
In <a href="matlab:matlab.lang.internal.introspective.errorDocCallback('Main>processExperiment', '/Users/jorge/Documents/qmul-phd-framework/src/matlab/Main.m', 220)" style="font-weight:bold">Main>processExperiment</a> (<a href="matlab: opentoline('/Users/jorge/Documents/qmul-phd-framework/src/matlab/Main.m',220,0)">line 220</a>)
[sample_neg] Final link counts (use_role_filter = 1):
    Train Positive: 716
    Train Negative: 716
    Test  Positive: 173
    Test  Negative: 173
    "Experiment "    "6"    " (strategy: "    "random"    ") — Running WLNM..."

Warning: [sample_neg] Not enough negatives. Reducing a...
> In <a href="matlab:matlab.lang.internal.introspective.errorDocCallback('sample_neg', '/Users/jorge/Documents/qmul-phd-framework/src/matlab/sample_neg.m', 82)" style="font-weight:bold">sample_neg</a> (<a href="matlab: opentoline('/Users/jorge/Documents/qmul-phd-framework/src/matlab/sample_neg.m',82,0)">line 82</a>)
In <a href="matlab:matlab.lang.internal.introspective.errorDocCallback('WLNM', '/Users/jorge/Documents/qmul-phd-framework/src/matlab/WLNM.m', 33)" style="font-weight:bold">WLNM</a> (<a href="matlab: opentoline('/Users/jorge/Documents/qmul-phd-framework/src/matlab/WLNM.m',33,0)">line 33</a>)
In <a href="matlab:matlab.lang.internal.introspective.errorDocCallback('Main>processExperiment', '/Users/jorge/Documents/qmul-phd-framework/src/matlab/Main.m', 220)" style="font-weight:bold">Main>processExperiment</a> (<a href="matlab: opentoline('/Users/jorge/Documents/qmul-phd-framework/src/matlab/Main.m',220,0)">line 220</a>)
Warning: [sample_neg] Using fallback: unfiltered negatives with a = 1.
> In <a href="matlab:matlab.lang.internal.introspective.errorDocCallback('sample_neg', '/Users/jorge/Documents/qmul-phd-framework/src/matlab/sample_neg.m', 87)" style="font-weight:bold">sample_neg</a> (<a href="matlab: opentoline('/Users/jorge/Documents/qmul-phd-framework/src/matlab/sample_neg.m',87,0)">line 87</a>)
In <a href="matlab:matlab.lang.internal.introspective.errorDocCallback('WLNM', '/Users/jorge/Documents/qmul-phd-framework/src/matlab/WLNM.m', 33)" style="font-weight:bold">WLNM</a> (<a href="matlab: opentoline('/Users/jorge/Documents/qmul-phd-framework/src/matlab/WLNM.m',33,0)">line 33</a>)
In <a href="matlab:matlab.lang.internal.introspective.errorDocCallback('Main>processExperiment', '/Users/jorge/Documents/qmul-phd-framework/src/matlab/Main.m', 220)" style="font-weight:bold">Main>processExperiment</a> (<a href="matlab: opentoline('/Users/jorge/Documents/qmul-phd-framework/src/matlab/Main.m',220,0)">line 220</a>)
[sample_neg] Final link counts (use_role_filter = 1):
    Train Positive: 716
    Train Negative: 716
    Test  Positive: 173
    Test  Negative: 173
[WLNM] Skipping DegreeCompute: non-degree-based strategy selected.
Encoding 1432 subgraphs (K = 10)...
Progress: 10% – Elapsed: 0.3s
    "Experiment "    "7"    " (strategy: "    "random"    ") — Running WLNM..."

Warning: [sample_neg] Not enough negatives. Reducing a...
> In <a href="matlab:matlab.lang.internal.introspective.errorDocCallback('sample_neg', '/Users/jorge/Documents/qmul-phd-framework/src/matlab/sample_neg.m', 82)" style="font-weight:bold">sample_neg</a> (<a href="matlab: opentoline('/Users/jorge/Documents/qmul-phd-framework/src/matlab/sample_neg.m',82,0)">line 82</a>)
In <a href="matlab:matlab.lang.internal.introspective.errorDocCallback('WLNM', '/Users/jorge/Documents/qmul-phd-framework/src/matlab/WLNM.m', 33)" style="font-weight:bold">WLNM</a> (<a href="matlab: opentoline('/Users/jorge/Documents/qmul-phd-framework/src/matlab/WLNM.m',33,0)">line 33</a>)
In <a href="matlab:matlab.lang.internal.introspective.errorDocCallback('Main>processExperiment', '/Users/jorge/Documents/qmul-phd-framework/src/matlab/Main.m', 220)" style="font-weight:bold">Main>processExperiment</a> (<a href="matlab: opentoline('/Users/jorge/Documents/qmul-phd-framework/src/matlab/Main.m',220,0)">line 220</a>)
Warning: [sample_neg] Using fallback: unfiltered negatives with a = 1.
> In <a href="matlab:matlab.lang.internal.introspective.errorDocCallback('sample_neg', '/Users/jorge/Documents/qmul-phd-framework/src/matlab/sample_neg.m', 87)" style="font-weight:bold">sample_neg</a> (<a href="matlab: opentoline('/Users/jorge/Documents/qmul-phd-framework/src/matlab/sample_neg.m',87,0)">line 87</a>)
In <a href="matlab:matlab.lang.internal.introspective.errorDocCallback('WLNM', '/Users/jorge/Documents/qmul-phd-framework/src/matlab/WLNM.m', 33)" style="font-weight:bold">WLNM</a> (<a href="matlab: opentoline('/Users/jorge/Documents/qmul-phd-framework/src/matlab/WLNM.m',33,0)">line 33</a>)
In <a href="matlab:matlab.lang.internal.introspective.errorDocCallback('Main>processExperiment', '/Users/jorge/Documents/qmul-phd-framework/src/matlab/Main.m', 220)" style="font-weight:bold">Main>processExperiment</a> (<a href="matlab: opentoline('/Users/jorge/Documents/qmul-phd-framework/src/matlab/Main.m',220,0)">line 220</a>)
[sample_neg] Final link counts (use_role_filter = 1):
    Train Positive: 716
    Train Negative: 716
    Test  Positive: 173
    Test  Negative: 173
[WLNM] Skipping DegreeCompute: non-degree-based strategy selected.
Encoding 1432 subgraphs (K = 10)...
Progress: 10% – Elapsed: 0.3s
    "Experiment "    "5"    " (strategy: "    "random"    ") — Running WLNM..."

Warning: [sample_neg] Not enough negatives. Reducing a...
> In <a href="matlab:matlab.lang.internal.introspective.errorDocCallback('sample_neg', '/Users/jorge/Documents/qmul-phd-framework/src/matlab/sample_neg.m', 82)" style="font-weight:bold">sample_neg</a> (<a href="matlab: opentoline('/Users/jorge/Documents/qmul-phd-framework/src/matlab/sample_neg.m',82,0)">line 82</a>)
In <a href="matlab:matlab.lang.internal.introspective.errorDocCallback('WLNM', '/Users/jorge/Documents/qmul-phd-framework/src/matlab/WLNM.m', 33)" style="font-weight:bold">WLNM</a> (<a href="matlab: opentoline('/Users/jorge/Documents/qmul-phd-framework/src/matlab/WLNM.m',33,0)">line 33</a>)
In <a href="matlab:matlab.lang.internal.introspective.errorDocCallback('Main>processExperiment', '/Users/jorge/Documents/qmul-phd-framework/src/matlab/Main.m', 220)" style="font-weight:bold">Main>processExperiment</a> (<a href="matlab: opentoline('/Users/jorge/Documents/qmul-phd-framework/src/matlab/Main.m',220,0)">line 220</a>)
Warning: [sample_neg] Using fallback: unfiltered negatives with a = 1.
> In <a href="matlab:matlab.lang.internal.introspective.errorDocCallback('sample_neg', '/Users/jorge/Documents/qmul-phd-framework/src/matlab/sample_neg.m', 87)" style="font-weight:bold">sample_neg</a> (<a href="matlab: opentoline('/Users/jorge/Documents/qmul-phd-framework/src/matlab/sample_neg.m',87,0)">line 87</a>)
In <a href="matlab:matlab.lang.internal.introspective.errorDocCallback('WLNM', '/Users/jorge/Documents/qmul-phd-framework/src/matlab/WLNM.m', 33)" style="font-weight:bold">WLNM</a> (<a href="matlab: opentoline('/Users/jorge/Documents/qmul-phd-framework/src/matlab/WLNM.m',33,0)">line 33</a>)
In <a href="matlab:matlab.lang.internal.introspective.errorDocCallback('Main>processExperiment', '/Users/jorge/Documents/qmul-phd-framework/src/matlab/Main.m', 220)" style="font-weight:bold">Main>processExperiment</a> (<a href="matlab: opentoline('/Users/jorge/Documents/qmul-phd-framework/src/matlab/Main.m',220,0)">line 220</a>)
[sample_neg] Final link counts (use_role_filter = 1):
    Train Positive: 716
    Train Negative: 716
    Test  Positive: 173
    Test  Negative: 173
[WLNM] Skipping DegreeCompute: non-degree-based strategy selected.
Encoding 1432 subgraphs (K = 10)...
[WLNM] Skipping DegreeCompute: non-degree-based strategy selected.
Encoding 1432 subgraphs (K = 10)...
Progress: 10% – Elapsed: 0.1s
Progress: 20% – Elapsed: 0.4s
Progress: 10% – Elapsed: 0.2s
Progress: 20% – Elapsed: 0.4s
Progress: 10% – Elapsed: 0.2s
Progress: 10% – Elapsed: 0.2s
Progress: 10% – Elapsed: 0.2s
Progress: 10% – Elapsed: 0.2s
Progress: 20% – Elapsed: 0.3s
Progress: 30% – Elapsed: 0.5s
Progress: 20% – Elapsed: 0.3s
Progress: 20% – Elapsed: 0.3s
Progress: 20% – Elapsed: 0.3s
Progress: 20% – Elapsed: 0.3s
Progress: 20% – Elapsed: 0.3s
Progress: 30% – Elapsed: 0.4s
Progress: 40% – Elapsed: 0.6s
Progress: 30% – Elapsed: 0.5s
Progress: 30% – Elapsed: 0.6s
Progress: 30% – Elapsed: 0.5s
Progress: 30% – Elapsed: 0.5s
Progress: 30% – Elapsed: 0.5s
Progress: 40% – Elapsed: 0.6s
Progress: 30% – Elapsed: 0.4s
Progress: 50% – Elapsed: 0.8s
Progress: 40% – Elapsed: 0.5s
Progress: 40% – Elapsed: 0.8s
Progress: 40% – Elapsed: 0.6s
Progress: 40% – Elapsed: 0.7s
Progress: 40% – Elapsed: 0.7s
Progress: 40% – Elapsed: 0.5s
Progress: 60% – Elapsed: 1.0s
Progress: 50% – Elapsed: 0.8s
Progress: 60% – Elapsed: 1.0s
Progress: 50% – Elapsed: 1.0s
Progress: 50% – Elapsed: 0.9s
Progress: 50% – Elapsed: 0.9s
Progress: 50% – Elapsed: 0.9s
Progress: 50% – Elapsed: 0.9s
Progress: 50% – Elapsed: 0.9s
Progress: 70% – Elapsed: 1.3s
Progress: 60% – Elapsed: 1.3s
Progress: 60% – Elapsed: 1.1s
Progress: 60% – Elapsed: 1.1s
Progress: 60% – Elapsed: 1.1s
Progress: 60% – Elapsed: 1.1s
Progress: 60% – Elapsed: 1.1s
Progress: 70% – Elapsed: 1.2s
Progress: 80% – Elapsed: 1.4s
Progress: 70% – Elapsed: 1.5s
Progress: 70% – Elapsed: 1.4s
Progress: 70% – Elapsed: 1.2s
Progress: 70% – Elapsed: 1.2s
Progress: 70% – Elapsed: 1.2s
Progress: 70% – Elapsed: 1.3s
Progress: 80% – Elapsed: 1.7s
Progress: 80% – Elapsed: 1.6s
Progress: 80% – Elapsed: 1.5s
Progress: 80% – Elapsed: 1.5s
Progress: 80% – Elapsed: 1.6s
Progress: 80% – Elapsed: 1.6s
Progress: 90% – Elapsed: 1.8s
Progress: 100% – Elapsed: 2.2s
Done. Total time: 2.2s
Encoding 346 subgraphs (K = 10)...
Progress: 80% – Elapsed: 1.8s
Progress: 90% – Elapsed: 2.0s
Progress: 90% – Elapsed: 1.9s
Progress: 90% – Elapsed: 1.9s
Progress: 90% – Elapsed: 1.9s
Progress: 90% – Elapsed: 2.1s
Progress: 100% – Elapsed: 2.2s
Done. Total time: 2.2s
Encoding 346 subgraphs (K = 10)...
Progress: 10% – Elapsed: 0.1s
Progress: 90% – Elapsed: 1.9s
Progress: 100% – Elapsed: 2.1s
Done. Total time: 2.1s
Encoding 346 subgraphs (K = 10)...
Progress: 90% – Elapsed: 2.0s
Progress: 100% – Elapsed: 2.1s
Done. Total time: 2.1s
Encoding 346 subgraphs (K = 10)...
Progress: 10% – Elapsed: 0.0s
Progress: 10% – Elapsed: 0.0s
Progress: 20% – Elapsed: 0.1s
Progress: 29% – Elapsed: 0.2s
Progress: 39% – Elapsed: 0.2s
Progress: 20% – Elapsed: 0.1s
Progress: 29% – Elapsed: 0.1s
Progress: 100% – Elapsed: 2.1s
Done. Total time: 2.1s
Encoding 346 subgraphs (K = 10)...
Progress: 10% – Elapsed: 0.1s
Progress: 20% – Elapsed: 0.1s
Progress: 29% – Elapsed: 0.1s
Progress: 39% – Elapsed: 0.2s
Progress: 100% – Elapsed: 2.2s
Done. Total time: 2.2s
Encoding 346 subgraphs (K = 10)...
Progress: 10% – Elapsed: 0.0s
Progress: 20% – Elapsed: 0.1s
Progress: 20% – Elapsed: 0.1s
Progress: 29% – Elapsed: 0.1s
Progress: 49% – Elapsed: 0.2s
Progress: 59% – Elapsed: 0.3s
Progress: 69% – Elapsed: 0.3s
Progress: 100% – Elapsed: 2.3s
Done. Total time: 2.3s
Encoding 346 subgraphs (K = 10)...
Progress: 39% – Elapsed: 0.2s
Progress: 49% – Elapsed: 0.3s
Progress: 49% – Elapsed: 0.2s
Progress: 10% – Elapsed: 0.0s
Progress: 20% – Elapsed: 0.1s
Progress: 29% – Elapsed: 0.1s
Progress: 29% – Elapsed: 0.1s
Progress: 39% – Elapsed: 0.2s
Progress: 100% – Elapsed: 2.3s
Done. Total time: 2.3s
Encoding 346 subgraphs (K = 10)...
Progress: 39% – Elapsed: 0.2s
Progress: 10% – Elapsed: 0.0s
Progress: 20% – Elapsed: 0.1s
Progress: 29% – Elapsed: 0.1s
Progress: 59% – Elapsed: 0.3s
Progress: 39% – Elapsed: 0.2s
Progress: 49% – Elapsed: 0.3s
Progress: 59% – Elapsed: 0.3s
Progress: 69% – Elapsed: 0.3s
Progress: 49% – Elapsed: 0.3s
Progress: 59% – Elapsed: 0.3s
Progress: 10% – Elapsed: 0.0s
Progress: 49% – Elapsed: 0.2s
Progress: 59% – Elapsed: 0.3s
Progress: 69% – Elapsed: 0.3s
Progress: 79% – Elapsed: 0.3s
Progress: 79% – Elapsed: 0.4s
Progress: 88% – Elapsed: 0.5s
Progress: 98% – Elapsed: 0.5s
Done. Total time: 0.5s
Progress: 39% – Elapsed: 0.1s
Progress: 49% – Elapsed: 0.2s
Progress: 59% – Elapsed: 0.2s
Progress: 69% – Elapsed: 0.2s
Progress: 69% – Elapsed: 0.4s
Progress: 79% – Elapsed: 0.4s
Progress: 88% – Elapsed: 0.5s
Progress: 98% – Elapsed: 0.5s
Done. Total time: 0.5s
Progress: 59% – Elapsed: 0.4s
Progress: 79% – Elapsed: 0.3s
Progress: 69% – Elapsed: 0.3s
Progress: 79% – Elapsed: 0.4s
Progress: 88% – Elapsed: 0.4s
Progress: 98% – Elapsed: 0.4s
Done. Total time: 0.4s
Progress: 20% – Elapsed: 0.2s
Progress: 88% – Elapsed: 0.4s
Progress: 98% – Elapsed: 0.4s
Done. Total time: 0.4s
|========================================================================================|
|  Epoch  |  Iteration  |  Time Elapsed  |  Mini-batch  |  Mini-batch  |  Base Learning  |
|         |             |   (hh:mm:ss)   |   Accuracy   |     Loss     |      Rate       |
|========================================================================================|
Progress: 79% – Elapsed: 0.3s
Progress: 88% – Elapsed: 0.3s
Progress: 98% – Elapsed: 0.3s
Done. Total time: 0.3s
|========================================================================================|
|  Epoch  |  Iteration  |  Time Elapsed  |  Mini-batch  |  Mini-batch  |  Base Learning  |
|         |             |   (hh:mm:ss)   |   Accuracy   |     Loss     |      Rate       |
|========================================================================================|
|       1 |           1 |       00:00:00 |       65.62% |       0.6804 |          0.1000 |
Progress: 69% – Elapsed: 0.4s
Progress: 79% – Elapsed: 0.5s
Progress: 88% – Elapsed: 0.5s
Progress: 98% – Elapsed: 0.5s
Done. Total time: 0.6s
Progress: 88% – Elapsed: 0.6s
|========================================================================================|
|  Epoch  |  Iteration  |  Time Elapsed  |  Mini-batch  |  Mini-batch  |  Base Learning  |
|         |             |   (hh:mm:ss)   |   Accuracy   |     Loss     |      Rate       |
|========================================================================================|
|       1 |           1 |       00:00:00 |       40.62% |       0.7181 |          0.1000 |
Progress: 29% – Elapsed: 0.4s
|       1 |           1 |       00:00:00 |       49.22% |       0.7343 |          0.1000 |
|========================================================================================|
|  Epoch  |  Iteration  |  Time Elapsed  |  Mini-batch  |  Mini-batch  |  Base Learning  |
|         |             |   (hh:mm:ss)   |   Accuracy   |     Loss     |      Rate       |
|========================================================================================|
|       1 |           1 |       00:00:00 |       45.31% |       0.7540 |          0.1000 |
Progress: 98% – Elapsed: 0.6s
Done. Total time: 0.6s
Progress: 39% – Elapsed: 0.5s
Progress: 49% – Elapsed: 0.6s
Progress: 59% – Elapsed: 0.6s
Progress: 69% – Elapsed: 0.6s
|========================================================================================|
|  Epoch  |  Iteration  |  Time Elapsed  |  Mini-batch  |  Mini-batch  |  Base Learning  |
|         |             |   (hh:mm:ss)   |   Accuracy   |     Loss     |      Rate       |
|========================================================================================|
|       1 |           1 |       00:00:00 |       46.88% |       0.7168 |          0.1000 |
|========================================================================================|
|  Epoch  |  Iteration  |  Time Elapsed  |  Mini-batch  |  Mini-batch  |  Base Learning  |
|         |             |   (hh:mm:ss)   |   Accuracy   |     Loss     |      Rate       |
|========================================================================================|
|       1 |           1 |       00:00:00 |       57.81% |       0.6864 |          0.1000 |
|========================================================================================|
|  Epoch  |  Iteration  |  Time Elapsed  |  Mini-batch  |  Mini-batch  |  Base Learning  |
|         |             |   (hh:mm:ss)   |   Accuracy   |     Loss     |      Rate       |
|========================================================================================|
|       1 |           1 |       00:00:00 |       39.06% |       0.7180 |          0.1000 |
Progress: 79% – Elapsed: 0.7s
|       5 |          50 |       00:00:00 |       81.25% |       0.4132 |          0.1000 |
|       5 |          50 |       00:00:00 |       75.78% |       0.4437 |          0.1000 |
|       5 |          50 |       00:00:00 |       76.56% |       0.4615 |          0.1000 |
|       5 |          50 |       00:00:00 |       82.81% |       0.3978 |          0.1000 |
|       5 |          50 |       00:00:00 |       83.59% |       0.3668 |          0.1000 |
|       5 |          50 |       00:00:00 |       82.03% |       0.3965 |          0.1000 |
|      10 |         100 |       00:00:00 |       87.50% |       0.3281 |          0.1000 |
Progress: 88% – Elapsed: 0.9s
Progress: 98% – Elapsed: 1.0s
Done. Total time: 1.0s
|      10 |         100 |       00:00:00 |       82.03% |       0.3967 |          0.1000 |
|      10 |         100 |       00:00:00 |       82.81% |       0.3802 |          0.1000 |
|      10 |         100 |       00:00:00 |       87.50% |       0.3069 |          0.1000 |
|       5 |          50 |       00:00:00 |       76.56% |       0.3881 |          0.1000 |
|========================================================================================|
|  Epoch  |  Iteration  |  Time Elapsed  |  Mini-batch  |  Mini-batch  |  Base Learning  |
|         |             |   (hh:mm:ss)   |   Accuracy   |     Loss     |      Rate       |
|========================================================================================|
|       1 |           1 |       00:00:00 |       48.44% |       0.7046 |          0.1000 |
|      10 |         100 |       00:00:00 |       88.28% |       0.2674 |          0.1000 |
|      14 |         150 |       00:00:01 |       89.84% |       0.2402 |          0.0900 |
|      14 |         150 |       00:00:01 |       83.59% |       0.3323 |          0.0900 |
|      10 |         100 |       00:00:00 |       88.28% |       0.3135 |          0.1000 |
|      10 |         100 |       00:00:00 |       87.50% |       0.3013 |          0.1000 |
|      14 |         150 |       00:00:00 |       90.62% |       0.2903 |          0.0900 |
|       5 |          50 |       00:00:00 |       89.06% |       0.3529 |          0.1000 |
|      14 |         150 |       00:00:01 |       91.41% |       0.2075 |          0.0900 |
|      14 |         150 |       00:00:01 |       87.50% |       0.2664 |          0.0900 |
|      19 |         200 |       00:00:01 |       90.62% |       0.2097 |          0.0900 |
|      14 |         150 |       00:00:01 |       84.38% |       0.3205 |          0.0900 |
|      14 |         150 |       00:00:01 |       86.72% |       0.3107 |          0.0900 |
|      19 |         200 |       00:00:01 |       88.28% |       0.2345 |          0.0900 |
|      10 |         100 |       00:00:00 |       80.47% |       0.3893 |          0.1000 |
|      19 |         200 |       00:00:01 |       93.75% |       0.1986 |          0.0900 |
|      23 |         250 |       00:00:01 |       94.53% |       0.1658 |          0.0810 |
|      19 |         200 |       00:00:01 |       87.50% |       0.3364 |          0.0900 |
|      23 |         250 |       00:00:01 |       95.31% |       0.1501 |          0.0810 |
|      19 |         200 |       00:00:01 |       89.84% |       0.2198 |          0.0900 |
|      23 |         250 |       00:00:01 |       86.72% |       0.2432 |          0.0810 |
|      19 |         200 |       00:00:01 |       92.97% |       0.1798 |          0.0900 |
|      23 |         250 |       00:00:01 |       86.72% |       0.3037 |          0.0810 |
|      28 |         300 |       00:00:01 |       89.06% |       0.2252 |          0.0810 |
|      19 |         200 |       00:00:01 |       85.94% |       0.2923 |          0.0900 |
|      23 |         250 |       00:00:01 |       85.94% |       0.2567 |          0.0810 |
|      23 |         250 |       00:00:01 |       89.84% |       0.2103 |          0.0810 |
|      28 |         300 |       00:00:01 |       89.06% |       0.2596 |          0.0810 |
|      14 |         150 |       00:00:00 |       88.28% |       0.2920 |          0.0900 |
|      23 |         250 |       00:00:01 |       88.28% |       0.2488 |          0.0810 |
|      19 |         200 |       00:00:01 |       88.28% |       0.2642 |          0.0900 |
|      28 |         300 |       00:00:01 |       88.28% |       0.2569 |          0.0810 |
|      28 |         300 |       00:00:02 |       87.50% |       0.2634 |          0.0810 |
|      32 |         350 |       00:00:02 |       96.09% |       0.0820 |          0.0729 |
|      28 |         300 |       00:00:01 |       92.19% |       0.1872 |          0.0810 |
|      28 |         300 |       00:00:01 |       86.72% |       0.2657 |          0.0810 |
|      32 |         350 |       00:00:02 |       88.28% |       0.2049 |          0.0729 |
|      32 |         350 |       00:00:02 |       92.19% |       0.1603 |          0.0729 |
|      28 |         300 |       00:00:02 |       91.41% |       0.2220 |          0.0810 |
|      32 |         350 |       00:00:01 |       94.53% |       0.1416 |          0.0729 |
|      37 |         400 |       00:00:02 |       92.97% |       0.1332 |          0.0729 |
|      23 |         250 |       00:00:01 |       89.84% |       0.2441 |          0.0810 |
|      32 |         350 |       00:00:02 |       91.41% |       0.2094 |          0.0729 |
|      32 |         350 |       00:00:02 |       92.19% |       0.1623 |          0.0729 |
|      37 |         400 |       00:00:02 |       94.53% |       0.1428 |          0.0729 |
|      37 |         400 |       00:00:02 |       91.41% |       0.1407 |          0.0729 |
|      32 |         350 |       00:00:02 |       93.75% |       0.1725 |          0.0729 |
|      41 |         450 |       00:00:02 |       91.41% |       0.2152 |          0.0656 |
|      28 |         300 |       00:00:01 |       97.66% |       0.1303 |          0.0810 |
|      37 |         400 |       00:00:02 |       92.97% |       0.1643 |          0.0729 |
|      37 |         400 |       00:00:02 |       92.97% |       0.1660 |          0.0729 |
|      41 |         450 |       00:00:02 |       98.44% |       0.0671 |          0.0656 |
|      41 |         450 |       00:00:03 |       89.84% |       0.2125 |          0.0656 |
|      37 |         400 |       00:00:02 |       92.97% |       0.1972 |          0.0729 |
|      41 |         450 |       00:00:02 |       93.75% |       0.1206 |          0.0656 |
|      37 |         400 |       00:00:02 |       92.19% |       0.1604 |          0.0729 |
|      32 |         350 |       00:00:02 |       94.53% |       0.1309 |          0.0729 |
|      41 |         450 |       00:00:02 |       92.19% |       0.1314 |          0.0656 |
|      41 |         450 |       00:00:03 |       96.09% |       0.1026 |          0.0656 |
|      46 |         500 |       00:00:03 |       91.41% |       0.1970 |          0.0656 |
|      46 |         500 |       00:00:02 |       96.88% |       0.1110 |          0.0656 |
|      41 |         450 |       00:00:02 |       94.53% |       0.1103 |          0.0656 |
|      46 |         500 |       00:00:03 |       90.62% |       0.1555 |          0.0656 |
|      37 |         400 |       00:00:02 |       93.75% |       0.1707 |          0.0729 |
|      46 |         500 |       00:00:03 |       95.31% |       0.1316 |          0.0656 |
|      46 |         500 |       00:00:03 |       90.62% |       0.2766 |          0.0656 |
|      50 |         550 |       00:00:03 |       94.53% |       0.1145 |          0.0656 |
|      46 |         500 |       00:00:03 |       96.09% |       0.1151 |          0.0656 |
|      50 |         550 |       00:00:03 |       96.09% |       0.1056 |          0.0656 |
|      46 |         500 |       00:00:03 |       95.31% |       0.1306 |          0.0656 |
|      41 |         450 |       00:00:02 |       93.75% |       0.1489 |          0.0656 |
|      50 |         550 |       00:00:03 |       95.31% |       0.1086 |          0.0656 |
|      50 |         550 |       00:00:03 |       92.19% |       0.1622 |          0.0656 |
|      55 |         600 |       00:00:03 |       92.19% |       0.1330 |          0.0590 |
|      50 |         550 |       00:00:03 |       92.97% |       0.1487 |          0.0656 |
|      55 |         600 |       00:00:03 |       96.88% |       0.0704 |          0.0590 |
|      55 |         600 |       00:00:03 |       92.19% |       0.1671 |          0.0590 |
|      50 |         550 |       00:00:03 |       93.75% |       0.1080 |          0.0656 |
|      55 |         600 |       00:00:03 |       89.84% |       0.1730 |          0.0590 |
|      50 |         550 |       00:00:03 |       93.75% |       0.1259 |          0.0656 |
|      46 |         500 |       00:00:02 |       92.97% |       0.1583 |          0.0656 |
|      55 |         600 |       00:00:03 |       95.31% |       0.2215 |          0.0590 |
|      60 |         650 |       00:00:03 |       97.66% |       0.0577 |          0.0590 |
|      55 |         600 |       00:00:03 |       92.19% |       0.1415 |          0.0590 |
|      60 |         650 |       00:00:03 |       92.97% |       0.1134 |          0.0590 |
|      50 |         550 |       00:00:03 |       97.66% |       0.0968 |          0.0656 |
|      60 |         650 |       00:00:04 |       88.28% |       0.2183 |          0.0590 |
|      55 |         600 |       00:00:03 |       91.41% |       0.1973 |          0.0590 |
|      60 |         650 |       00:00:03 |       95.31% |       0.0981 |          0.0590 |
|      60 |         650 |       00:00:04 |       96.09% |       0.0922 |          0.0590 |
|      64 |         700 |       00:00:04 |       93.75% |       0.1531 |          0.0531 |
|      64 |         700 |       00:00:04 |       95.31% |       0.1260 |          0.0531 |
|      60 |         650 |       00:00:04 |       97.66% |       0.0809 |          0.0590 |
|      64 |         700 |       00:00:04 |       94.53% |       0.1558 |          0.0531 |
|      55 |         600 |       00:00:03 |       92.97% |       0.1337 |          0.0590 |
|      64 |         700 |       00:00:04 |       96.09% |       0.1116 |          0.0531 |
|      60 |         650 |       00:00:04 |       92.97% |       0.1371 |          0.0590 |
|      64 |         700 |       00:00:04 |       92.97% |       0.2518 |          0.0531 |
|      69 |         750 |       00:00:04 |       97.66% |       0.0735 |          0.0531 |
|      69 |         750 |       00:00:04 |       89.84% |       0.2173 |          0.0531 |
|      64 |         700 |       00:00:04 |       94.53% |       0.1172 |          0.0531 |
|      69 |         750 |       00:00:04 |       96.09% |       0.1083 |          0.0531 |
|      60 |         650 |       00:00:03 |       89.84% |       0.2406 |          0.0590 |
|      69 |         750 |       00:00:04 |       92.19% |       0.1921 |          0.0531 |
|      64 |         700 |       00:00:04 |       83.59% |       0.3301 |          0.0531 |
|      69 |         750 |       00:00:04 |       92.19% |       0.2145 |          0.0531 |
|      73 |         800 |       00:00:04 |       94.53% |       0.1308 |          0.0478 |
|      69 |         750 |       00:00:04 |       95.31% |       0.1147 |          0.0531 |
|      73 |         800 |       00:00:04 |       89.84% |       0.2076 |          0.0478 |
|      69 |         750 |       00:00:04 |       92.97% |       0.1133 |          0.0531 |
|      73 |         800 |       00:00:04 |       95.31% |       0.1110 |          0.0478 |
|      73 |         800 |       00:00:05 |       90.62% |       0.1424 |          0.0478 |
|      73 |         800 |       00:00:04 |       92.19% |       0.1281 |          0.0478 |
|      78 |         850 |       00:00:05 |       92.19% |       0.1966 |          0.0478 |
|      73 |         800 |       00:00:04 |       88.28% |       0.2469 |          0.0478 |
|      78 |         850 |       00:00:04 |       91.41% |       0.1755 |          0.0478 |
|      78 |         850 |       00:00:05 |       93.75% |       0.1525 |          0.0478 |
|      64 |         700 |       00:00:04 |       92.19% |       0.1924 |          0.0531 |
|      69 |         750 |       00:00:04 |       96.88% |       0.1051 |          0.0531 |
|      73 |         800 |       00:00:05 |       94.53% |       0.1135 |          0.0478 |
|      78 |         850 |       00:00:04 |       92.97% |       0.2281 |          0.0478 |
|      78 |         850 |       00:00:05 |       94.53% |       0.1303 |          0.0478 |
|      78 |         850 |       00:00:05 |       89.06% |       0.1777 |          0.0478 |
|      78 |         850 |       00:00:05 |       94.53% |       0.1204 |          0.0478 |
|      82 |         900 |       00:00:05 |       94.53% |       0.1160 |          0.0430 |
|      82 |         900 |       00:00:05 |       96.88% |       0.0895 |          0.0430 |
|      82 |         900 |       00:00:05 |       95.31% |       0.1021 |          0.0430 |
|      82 |         900 |       00:00:05 |       96.88% |       0.0751 |          0.0430 |
|      82 |         900 |       00:00:05 |       96.88% |       0.0649 |          0.0430 |
|      82 |         900 |       00:00:05 |       96.09% |       0.1250 |          0.0430 |
|      87 |         950 |       00:00:05 |       94.53% |       0.1114 |          0.0430 |
|      73 |         800 |       00:00:05 |       96.09% |       0.0996 |          0.0478 |
|      82 |         900 |       00:00:05 |       98.44% |       0.0478 |          0.0430 |
|      87 |         950 |       00:00:06 |       98.44% |       0.0498 |          0.0430 |
|      87 |         950 |       00:00:05 |       96.09% |       0.0733 |          0.0430 |
|      87 |         950 |       00:00:05 |       96.88% |       0.0675 |          0.0430 |
|      87 |         950 |       00:00:05 |       96.88% |       0.0690 |          0.0430 |
|      91 |        1000 |       00:00:06 |       99.22% |       0.0578 |          0.0387 |
|      87 |         950 |       00:00:05 |       95.31% |       0.1102 |          0.0430 |
|      91 |        1000 |       00:00:05 |       96.88% |       0.0529 |          0.0387 |
|      91 |        1000 |       00:00:06 |       91.41% |       0.1345 |          0.0387 |
|      78 |         850 |       00:00:05 |       97.66% |       0.0607 |          0.0478 |
|      87 |         950 |       00:00:06 |       96.88% |       0.0803 |          0.0430 |
|      91 |        1000 |       00:00:06 |       98.44% |       0.0600 |          0.0387 |
|      91 |        1000 |       00:00:06 |       99.22% |       0.0341 |          0.0387 |
|      96 |        1050 |       00:00:06 |       96.09% |       0.0705 |          0.0387 |
|      91 |        1000 |       00:00:06 |       97.66% |       0.0671 |          0.0387 |
|      96 |        1050 |       00:00:06 |       96.88% |       0.0761 |          0.0387 |
|      82 |         900 |       00:00:05 |       96.88% |       0.0783 |          0.0430 |
|      91 |        1000 |       00:00:06 |       94.53% |       0.1036 |          0.0387 |
|     100 |        1100 |       00:00:06 |       94.53% |       0.0868 |          0.0387 |
|      96 |        1050 |       00:00:06 |       96.88% |       0.0645 |          0.0387 |
|      96 |        1050 |       00:00:06 |       93.75% |       0.0965 |          0.0387 |
|      96 |        1050 |       00:00:06 |       96.88% |       0.0758 |          0.0387 |
|      96 |        1050 |       00:00:06 |       96.88% |       0.0736 |          0.0387 |
|      96 |        1050 |       00:00:06 |       96.09% |       0.0624 |          0.0387 |
|     100 |        1100 |       00:00:06 |       95.31% |       0.0883 |          0.0387 |
|     100 |        1100 |       00:00:06 |       96.09% |       0.0822 |          0.0387 |
|      87 |         950 |       00:00:05 |       97.66% |       0.0521 |          0.0430 |
|     100 |        1100 |       00:00:07 |       95.31% |       0.0868 |          0.0387 |
|     100 |        1100 |       00:00:06 |       96.88% |       0.0711 |          0.0387 |
|     105 |        1150 |       00:00:07 |       96.09% |       0.0783 |          0.0349 |
|     105 |        1150 |       00:00:06 |       94.53% |       0.0953 |          0.0349 |
|     100 |        1100 |       00:00:06 |       96.09% |       0.0692 |          0.0387 |
|     105 |        1150 |       00:00:07 |       98.44% |       0.0282 |          0.0349 |
|      91 |        1000 |       00:00:06 |       96.88% |       0.0555 |          0.0387 |
|     100 |        1100 |       00:00:06 |       96.88% |       0.0696 |          0.0387 |
|     105 |        1150 |       00:00:07 |       96.09% |       0.0777 |          0.0349 |
|     110 |        1200 |       00:00:06 |       96.09% |       0.0819 |          0.0349 |
|      96 |        1050 |       00:00:06 |       94.53% |       0.0998 |          0.0387 |
|     105 |        1150 |       00:00:07 |       97.66% |       0.0443 |          0.0349 |
|     105 |        1150 |       00:00:07 |       93.75% |       0.1105 |          0.0349 |
|     110 |        1200 |       00:00:07 |       97.66% |       0.0446 |          0.0349 |
|     105 |        1150 |       00:00:07 |       95.31% |       0.0977 |          0.0349 |
|     110 |        1200 |       00:00:07 |       96.09% |       0.0838 |          0.0349 |
|     110 |        1200 |       00:00:07 |       97.66% |       0.0521 |          0.0349 |
|     110 |        1200 |       00:00:07 |       92.97% |       0.1207 |          0.0349 |
|     110 |        1200 |       00:00:07 |       96.09% |       0.0760 |          0.0349 |
|     114 |        1250 |       00:00:07 |       95.31% |       0.0977 |          0.0314 |
|     114 |        1250 |       00:00:07 |       96.88% |       0.0799 |          0.0314 |
|     100 |        1100 |       00:00:06 |       98.44% |       0.0539 |          0.0387 |
|     114 |        1250 |       00:00:07 |       96.09% |       0.0790 |          0.0314 |
|     119 |        1300 |       00:00:07 |       97.66% |       0.0601 |          0.0314 |
|     110 |        1200 |       00:00:07 |       96.88% |       0.0661 |          0.0349 |
|     114 |        1250 |       00:00:07 |       97.66% |       0.0641 |          0.0314 |
|     114 |        1250 |       00:00:07 |       94.53% |       0.0881 |          0.0314 |
|     114 |        1250 |       00:00:07 |       96.09% |       0.0632 |          0.0314 |
|     119 |        1300 |       00:00:07 |       96.09% |       0.0834 |          0.0314 |
|     114 |        1250 |       00:00:07 |       95.31% |       0.0919 |          0.0314 |
|     119 |        1300 |       00:00:07 |       97.66% |       0.0593 |          0.0314 |
|     105 |        1150 |       00:00:07 |       96.09% |       0.0705 |          0.0349 |
|     119 |        1300 |       00:00:07 |       93.75% |       0.0970 |          0.0314 |
|     119 |        1300 |       00:00:08 |       97.66% |       0.0593 |          0.0314 |
|     119 |        1300 |       00:00:07 |       96.09% |       0.0635 |          0.0314 |
|     123 |        1350 |       00:00:07 |       96.09% |       0.1073 |          0.0282 |
|     119 |        1300 |       00:00:07 |       93.75% |       0.0985 |          0.0314 |
|     123 |        1350 |       00:00:08 |       99.22% |       0.0572 |          0.0282 |
|     110 |        1200 |       00:00:07 |       93.75% |       0.1124 |          0.0349 |
|     123 |        1350 |       00:00:08 |       90.62% |       0.1196 |          0.0282 |
|     123 |        1350 |       00:00:08 |       97.66% |       0.0668 |          0.0282 |
|     123 |        1350 |       00:00:08 |       95.31% |       0.0997 |          0.0282 |
|     128 |        1400 |       00:00:07 |       97.66% |       0.0511 |          0.0282 |
|     114 |        1250 |       00:00:07 |       92.97% |       0.1043 |          0.0314 |
|     123 |        1350 |       00:00:08 |       96.09% |       0.0843 |          0.0282 |
|     128 |        1400 |       00:00:08 |       95.31% |       0.0889 |          0.0282 |
|     128 |        1400 |       00:00:08 |       96.09% |       0.0918 |          0.0282 |
|     123 |        1350 |       00:00:08 |       96.88% |       0.0715 |          0.0282 |
|     128 |        1400 |       00:00:08 |       94.53% |       0.0958 |          0.0282 |
|     128 |        1400 |       00:00:08 |       93.75% |       0.1157 |          0.0282 |
|     119 |        1300 |       00:00:08 |       96.09% |       0.0854 |          0.0314 |
|     128 |        1400 |       00:00:08 |       94.53% |       0.1000 |          0.0282 |
|     132 |        1450 |       00:00:08 |       96.88% |       0.0519 |          0.0254 |
|     132 |        1450 |       00:00:08 |       97.66% |       0.0512 |          0.0254 |
|     128 |        1400 |       00:00:08 |       94.53% |       0.1043 |          0.0282 |
|     132 |        1450 |       00:00:08 |       96.09% |       0.0683 |          0.0254 |
|     123 |        1350 |       00:00:08 |       97.66% |       0.0639 |          0.0282 |
|     132 |        1450 |       00:00:09 |       96.09% |       0.0735 |          0.0254 |
|     132 |        1450 |       00:00:08 |       97.66% |       0.0619 |          0.0254 |
|     132 |        1450 |       00:00:09 |       98.44% |       0.0391 |          0.0254 |
|     137 |        1500 |       00:00:09 |       95.31% |       0.0636 |          0.0254 |
|     137 |        1500 |       00:00:08 |       96.09% |       0.0726 |          0.0254 |
|     132 |        1450 |       00:00:08 |       96.88% |       0.0642 |          0.0254 |
|     137 |        1500 |       00:00:09 |       96.88% |       0.0557 |          0.0254 |
|     128 |        1400 |       00:00:08 |       98.44% |       0.0471 |          0.0282 |
|     137 |        1500 |       00:00:09 |       98.44% |       0.0443 |          0.0254 |
|     137 |        1500 |       00:00:09 |       96.88% |       0.0601 |          0.0254 |
|     141 |        1550 |       00:00:09 |       99.22% |       0.0560 |          0.0229 |
|     141 |        1550 |       00:00:09 |       97.66% |       0.0559 |          0.0229 |
|     137 |        1500 |       00:00:09 |       93.75% |       0.0946 |          0.0254 |
|     141 |        1550 |       00:00:09 |       92.19% |       0.1089 |          0.0229 |
|     137 |        1500 |       00:00:09 |       96.88% |       0.0695 |          0.0254 |
|     141 |        1550 |       00:00:09 |       98.44% |       0.0524 |          0.0229 |
|     141 |        1550 |       00:00:09 |       99.22% |       0.0311 |          0.0229 |
|     132 |        1450 |       00:00:09 |       96.88% |       0.0688 |          0.0254 |
|     141 |        1550 |       00:00:09 |       94.53% |       0.0931 |          0.0229 |
|     146 |        1600 |       00:00:10 |       96.09% |       0.0658 |          0.0229 |
|     146 |        1600 |       00:00:09 |       96.09% |       0.0635 |          0.0229 |
|     141 |        1550 |       00:00:09 |       96.09% |       0.0503 |          0.0229 |
|     146 |        1600 |       00:00:10 |       94.53% |       0.0858 |          0.0229 |
|     146 |        1600 |       00:00:10 |       96.88% |       0.0597 |          0.0229 |
|     146 |        1600 |       00:00:10 |       96.09% |       0.0545 |          0.0229 |
|     137 |        1500 |       00:00:09 |       97.66% |       0.0419 |          0.0254 |
|     146 |        1600 |       00:00:10 |       96.88% |       0.0698 |          0.0229 |
|     150 |        1650 |       00:00:10 |       96.88% |       0.0694 |          0.0229 |
|     150 |        1650 |       00:00:10 |       96.09% |       0.0807 |          0.0229 |
|     150 |        1650 |       00:00:10 |       94.53% |       0.0799 |          0.0229 |
|     146 |        1600 |       00:00:10 |       96.88% |       0.0565 |          0.0229 |
|     150 |        1650 |       00:00:10 |       97.66% |       0.0705 |          0.0229 |
|     150 |        1650 |       00:00:10 |       95.31% |       0.0798 |          0.0229 |
|     150 |        1650 |       00:00:10 |       97.66% |       0.0580 |          0.0229 |
|     141 |        1550 |       00:00:09 |       96.88% |       0.0441 |          0.0229 |
|     150 |        1650 |       00:00:10 |       96.88% |       0.0679 |          0.0229 |
|     155 |        1700 |       00:00:10 |       96.09% |       0.0678 |          0.0206 |
|     155 |        1700 |       00:00:10 |       94.53% |       0.0858 |          0.0206 |
|     155 |        1700 |       00:00:10 |       95.31% |       0.0936 |          0.0206 |
|     155 |        1700 |       00:00:10 |       99.22% |       0.0244 |          0.0206 |
|     146 |        1600 |       00:00:10 |       94.53% |       0.0950 |          0.0229 |
|     155 |        1700 |       00:00:11 |       96.09% |       0.0673 |          0.0206 |
|     155 |        1700 |       00:00:10 |       93.75% |       0.1085 |          0.0206 |
|     160 |        1750 |       00:00:11 |       98.44% |       0.0399 |          0.0206 |
|     160 |        1750 |       00:00:10 |       96.88% |       0.0597 |          0.0206 |
|     155 |        1700 |       00:00:11 |       98.44% |       0.0366 |          0.0206 |
|     160 |        1750 |       00:00:11 |       92.97% |       0.1100 |          0.0206 |
|     160 |        1750 |       00:00:11 |       96.09% |       0.0701 |          0.0206 |
|     160 |        1750 |       00:00:10 |       96.09% |       0.0658 |          0.0206 |
|     160 |        1750 |       00:00:11 |       96.88% |       0.0767 |          0.0206 |
|     150 |        1650 |       00:00:10 |       98.44% |       0.0497 |          0.0229 |
|     164 |        1800 |       00:00:11 |       95.31% |       0.0854 |          0.0185 |
|     164 |        1800 |       00:00:11 |       96.09% |       0.0794 |          0.0185 |
|     160 |        1750 |       00:00:11 |       97.66% |       0.0517 |          0.0206 |
|     164 |        1800 |       00:00:11 |       96.09% |       0.0605 |          0.0185 |
|     164 |        1800 |       00:00:11 |       96.09% |       0.0775 |          0.0185 |
|     164 |        1800 |       00:00:11 |       95.31% |       0.0911 |          0.0185 |
|     155 |        1700 |       00:00:10 |       96.09% |       0.0670 |          0.0206 |
|     164 |        1800 |       00:00:11 |       94.53% |       0.0836 |          0.0185 |
|     169 |        1850 |       00:00:11 |       93.75% |       0.0891 |          0.0185 |
|     169 |        1850 |       00:00:11 |       97.66% |       0.0576 |          0.0185 |
|     164 |        1800 |       00:00:11 |       96.88% |       0.0594 |          0.0185 |
|     169 |        1850 |       00:00:12 |       97.66% |       0.0603 |          0.0185 |
|     169 |        1850 |       00:00:11 |       96.09% |       0.0614 |          0.0185 |
|     169 |        1850 |       00:00:11 |       96.09% |       0.0821 |          0.0185 |
|     169 |        1850 |       00:00:11 |       97.66% |       0.0584 |          0.0185 |
|     160 |        1750 |       00:00:11 |       92.97% |       0.1022 |          0.0206 |
|     169 |        1850 |       00:00:12 |       94.53% |       0.0862 |          0.0185 |
|     173 |        1900 |       00:00:11 |       96.88% |       0.0708 |          0.0167 |
|     173 |        1900 |       00:00:12 |       96.09% |       0.0790 |          0.0167 |
|     173 |        1900 |       00:00:12 |       92.19% |       0.1121 |          0.0167 |
|     173 |        1900 |       00:00:12 |       96.88% |       0.0637 |          0.0167 |
|     173 |        1900 |       00:00:12 |       95.31% |       0.0987 |          0.0167 |
|     173 |        1900 |       00:00:12 |       96.09% |       0.0920 |          0.0167 |
|     173 |        1900 |       00:00:12 |       98.44% |       0.0579 |          0.0167 |
|     164 |        1800 |       00:00:11 |       92.97% |       0.1054 |          0.0185 |
|     178 |        1950 |       00:00:12 |       96.09% |       0.0860 |          0.0167 |
|     178 |        1950 |       00:00:12 |       93.75% |       0.0962 |          0.0167 |
|     178 |        1950 |       00:00:13 |       94.53% |       0.0829 |          0.0167 |
|     178 |        1950 |       00:00:12 |       93.75% |       0.0974 |          0.0167 |
|     178 |        1950 |       00:00:13 |       92.19% |       0.1055 |          0.0167 |
|     178 |        1950 |       00:00:13 |       95.31% |       0.0863 |          0.0167 |
|     178 |        1950 |       00:00:12 |       96.88% |       0.0473 |          0.0167 |
|     182 |        2000 |       00:00:12 |       96.09% |       0.0609 |          0.0150 |
|     182 |        2000 |       00:00:13 |       96.09% |       0.0664 |          0.0150 |
|     169 |        1850 |       00:00:12 |       96.88% |       0.0826 |          0.0185 |
|     182 |        2000 |       00:00:13 |       96.88% |       0.0510 |          0.0150 |
|     182 |        2000 |       00:00:13 |       98.44% |       0.0374 |          0.0150 |
|     182 |        2000 |       00:00:13 |       96.88% |       0.0689 |          0.0150 |
|     182 |        2000 |       00:00:13 |       97.66% |       0.0595 |          0.0150 |
|     182 |        2000 |       00:00:13 |       97.66% |       0.0487 |          0.0150 |
|     173 |        1900 |       00:00:12 |       97.66% |       0.0579 |          0.0167 |
|     187 |        2050 |       00:00:13 |       96.88% |       0.0655 |          0.0150 |
|     187 |        2050 |       00:00:13 |       96.09% |       0.0682 |          0.0150 |
|     187 |        2050 |       00:00:13 |       92.97% |       0.0939 |          0.0150 |
|     187 |        2050 |       00:00:13 |       96.88% |       0.0533 |          0.0150 |
|     178 |        1950 |       00:00:12 |       98.44% |       0.0467 |          0.0167 |
|     187 |        2050 |       00:00:13 |       96.88% |       0.0425 |          0.0150 |
|     187 |        2050 |       00:00:13 |       96.88% |       0.0582 |          0.0150 |
|     187 |        2050 |       00:00:13 |       95.31% |       0.0622 |          0.0150 |
|     191 |        2100 |       00:00:14 |       97.66% |       0.0511 |          0.0135 |
|     191 |        2100 |       00:00:13 |       99.22% |       0.0306 |          0.0135 |
|     191 |        2100 |       00:00:14 |       98.44% |       0.0528 |          0.0135 |
|     191 |        2100 |       00:00:13 |       97.66% |       0.0562 |          0.0135 |
|     191 |        2100 |       00:00:13 |       96.88% |       0.0483 |          0.0135 |
|     191 |        2100 |       00:00:13 |       93.75% |       0.0969 |          0.0135 |
|     182 |        2000 |       00:00:13 |       97.66% |       0.0668 |          0.0150 |
|     191 |        2100 |       00:00:13 |       94.53% |       0.0925 |          0.0135 |
|     196 |        2150 |       00:00:14 |       96.09% |       0.0617 |          0.0135 |
|     196 |        2150 |       00:00:13 |       96.88% |       0.0551 |          0.0135 |
|     196 |        2150 |       00:00:14 |       94.53% |       0.0851 |          0.0135 |
|     196 |        2150 |       00:00:14 |       96.88% |       0.0684 |          0.0135 |
|     196 |        2150 |       00:00:14 |       97.66% |       0.0576 |          0.0135 |
|     196 |        2150 |       00:00:14 |       96.09% |       0.0537 |          0.0135 |
|     196 |        2150 |       00:00:14 |       97.66% |       0.0514 |          0.0135 |
|     200 |        2200 |       00:00:14 |       97.66% |       0.0560 |          0.0135 |
|========================================================================================|
Training finished: Max epochs completed.
|     187 |        2050 |       00:00:13 |       97.66% |       0.0400 |          0.0150 |
|     200 |        2200 |       00:00:14 |       97.66% |       0.0684 |          0.0135 |
|========================================================================================|
Training finished: Max epochs completed.
|     200 |        2200 |       00:00:14 |       96.88% |       0.0667 |          0.0135 |
|========================================================================================|
Training finished: Max epochs completed.
|     200 |        2200 |       00:00:14 |       95.31% |       0.0786 |          0.0135 |
|========================================================================================|
Training finished: Max epochs completed.
Best Threshold: 0.30, Precision: 0.8168, Recall: 0.9017, F1-Score: 0.8571
AUC: 0.9178
Best Threshold: 0.55, Precision: 0.8834, Recall: 0.8324, F1-Score: 0.8571
AUC: 0.9198
|     191 |        2100 |       00:00:13 |       96.88% |       0.0464 |          0.0135 |
Best Threshold: 0.50, Precision: 0.8580, Recall: 0.8728, F1-Score: 0.8653
AUC: 0.9434
|     200 |        2200 |       00:00:14 |       95.31% |       0.0789 |          0.0135 |
|========================================================================================|
Training finished: Max epochs completed.
|     200 |        2200 |       00:00:14 |       96.88% |       0.0684 |          0.0135 |
|========================================================================================|
Training finished: Max epochs completed.
|     200 |        2200 |       00:00:14 |       94.53% |       0.0804 |          0.0135 |
|========================================================================================|
Training finished: Max epochs completed.
Best Threshold: 0.10, Precision: 0.8187, Recall: 0.9133, F1-Score: 0.8634
AUC: 0.9315
Best Threshold: 0.45, Precision: 0.8736, Recall: 0.8786, F1-Score: 0.8761
AUC: 0.9445
Best Threshold: 0.10, Precision: 0.8438, Recall: 0.9364, F1-Score: 0.8877
AUC: 0.9327
  Columns 1 through 4

    "Experiment "    "10"    " (strategy: "    "random"

  Column 5

    ") — Running WLNM..."

Warning: [sample_neg] Not enough negatives. Reducing a...
> In <a href="matlab:matlab.lang.internal.introspective.errorDocCallback('sample_neg', '/Users/jorge/Documents/qmul-phd-framework/src/matlab/sample_neg.m', 82)" style="font-weight:bold">sample_neg</a> (<a href="matlab: opentoline('/Users/jorge/Documents/qmul-phd-framework/src/matlab/sample_neg.m',82,0)">line 82</a>)
In <a href="matlab:matlab.lang.internal.introspective.errorDocCallback('WLNM', '/Users/jorge/Documents/qmul-phd-framework/src/matlab/WLNM.m', 33)" style="font-weight:bold">WLNM</a> (<a href="matlab: opentoline('/Users/jorge/Documents/qmul-phd-framework/src/matlab/WLNM.m',33,0)">line 33</a>)
In <a href="matlab:matlab.lang.internal.introspective.errorDocCallback('Main>processExperiment', '/Users/jorge/Documents/qmul-phd-framework/src/matlab/Main.m', 220)" style="font-weight:bold">Main>processExperiment</a> (<a href="matlab: opentoline('/Users/jorge/Documents/qmul-phd-framework/src/matlab/Main.m',220,0)">line 220</a>)
Warning: [sample_neg] Using fallback: unfiltered negatives with a = 1.
> In <a href="matlab:matlab.lang.internal.introspective.errorDocCallback('sample_neg', '/Users/jorge/Documents/qmul-phd-framework/src/matlab/sample_neg.m', 87)" style="font-weight:bold">sample_neg</a> (<a href="matlab: opentoline('/Users/jorge/Documents/qmul-phd-framework/src/matlab/sample_neg.m',87,0)">line 87</a>)
In <a href="matlab:matlab.lang.internal.introspective.errorDocCallback('WLNM', '/Users/jorge/Documents/qmul-phd-framework/src/matlab/WLNM.m', 33)" style="font-weight:bold">WLNM</a> (<a href="matlab: opentoline('/Users/jorge/Documents/qmul-phd-framework/src/matlab/WLNM.m',33,0)">line 33</a>)
In <a href="matlab:matlab.lang.internal.introspective.errorDocCallback('Main>processExperiment', '/Users/jorge/Documents/qmul-phd-framework/src/matlab/Main.m', 220)" style="font-weight:bold">Main>processExperiment</a> (<a href="matlab: opentoline('/Users/jorge/Documents/qmul-phd-framework/src/matlab/Main.m',220,0)">line 220</a>)
[sample_neg] Final link counts (use_role_filter = 1):
    Train Positive: 716
    Train Negative: 716
    Test  Positive: 173
    Test  Negative: 173
[WLNM] Skipping DegreeCompute: non-degree-based strategy selected.
Encoding 1432 subgraphs (K = 10)...
    "Experiment "    "9"    " (strategy: "    "random"    ") — Running WLNM..."

Warning: [sample_neg] Not enough negatives. Reducing a...
> In <a href="matlab:matlab.lang.internal.introspective.errorDocCallback('sample_neg', '/Users/jorge/Documents/qmul-phd-framework/src/matlab/sample_neg.m', 82)" style="font-weight:bold">sample_neg</a> (<a href="matlab: opentoline('/Users/jorge/Documents/qmul-phd-framework/src/matlab/sample_neg.m',82,0)">line 82</a>)
In <a href="matlab:matlab.lang.internal.introspective.errorDocCallback('WLNM', '/Users/jorge/Documents/qmul-phd-framework/src/matlab/WLNM.m', 33)" style="font-weight:bold">WLNM</a> (<a href="matlab: opentoline('/Users/jorge/Documents/qmul-phd-framework/src/matlab/WLNM.m',33,0)">line 33</a>)
In <a href="matlab:matlab.lang.internal.introspective.errorDocCallback('Main>processExperiment', '/Users/jorge/Documents/qmul-phd-framework/src/matlab/Main.m', 220)" style="font-weight:bold">Main>processExperiment</a> (<a href="matlab: opentoline('/Users/jorge/Documents/qmul-phd-framework/src/matlab/Main.m',220,0)">line 220</a>)
Warning: [sample_neg] Using fallback: unfiltered negatives with a = 1.
> In <a href="matlab:matlab.lang.internal.introspective.errorDocCallback('sample_neg', '/Users/jorge/Documents/qmul-phd-framework/src/matlab/sample_neg.m', 87)" style="font-weight:bold">sample_neg</a> (<a href="matlab: opentoline('/Users/jorge/Documents/qmul-phd-framework/src/matlab/sample_neg.m',87,0)">line 87</a>)
In <a href="matlab:matlab.lang.internal.introspective.errorDocCallback('WLNM', '/Users/jorge/Documents/qmul-phd-framework/src/matlab/WLNM.m', 33)" style="font-weight:bold">WLNM</a> (<a href="matlab: opentoline('/Users/jorge/Documents/qmul-phd-framework/src/matlab/WLNM.m',33,0)">line 33</a>)
In <a href="matlab:matlab.lang.internal.introspective.errorDocCallback('Main>processExperiment', '/Users/jorge/Documents/qmul-phd-framework/src/matlab/Main.m', 220)" style="font-weight:bold">Main>processExperiment</a> (<a href="matlab: opentoline('/Users/jorge/Documents/qmul-phd-framework/src/matlab/Main.m',220,0)">line 220</a>)
[sample_neg] Final link counts (use_role_filter = 1):
    Train Positive: 716
    Train Negative: 716
    Test  Positive: 173
    Test  Negative: 173
[WLNM] Skipping DegreeCompute: non-degree-based strategy selected.
Encoding 1432 subgraphs (K = 10)...
Best Threshold: 0.50, Precision: 0.8333, Recall: 0.8960, F1-Score: 0.8635
AUC: 0.9240
Progress: 10% – Elapsed: 0.2s
Progress: 10% – Elapsed: 0.2s
|     196 |        2150 |       00:00:14 |       94.53% |       0.0947 |          0.0135 |
Progress: 20% – Elapsed: 0.3s
Progress: 30% – Elapsed: 0.4s
Progress: 20% – Elapsed: 0.3s
Progress: 30% – Elapsed: 0.4s
|     200 |        2200 |       00:00:15 |       98.44% |       0.0512 |          0.0135 |
|========================================================================================|
Training finished: Max epochs completed.
Best Threshold: 0.40, Precision: 0.8172, Recall: 0.8786, F1-Score: 0.8468
AUC: 0.8944
Progress: 40% – Elapsed: 0.5s
Progress: 40% – Elapsed: 0.6s
Progress: 50% – Elapsed: 0.6s
Progress: 60% – Elapsed: 0.7s
Progress: 50% – Elapsed: 0.7s
Progress: 60% – Elapsed: 0.8s
Progress: 70% – Elapsed: 0.8s
Progress: 70% – Elapsed: 0.8s
Progress: 80% – Elapsed: 0.9s
Progress: 90% – Elapsed: 0.9s
Progress: 80% – Elapsed: 0.9s
Progress: 100% – Elapsed: 1.0s
Done. Total time: 1.0s
Encoding 346 subgraphs (K = 10)...
Progress: 10% – Elapsed: 0.0s
Progress: 20% – Elapsed: 0.1s
Progress: 29% – Elapsed: 0.1s
Progress: 39% – Elapsed: 0.1s
Progress: 49% – Elapsed: 0.1s
Progress: 59% – Elapsed: 0.1s
Progress: 90% – Elapsed: 1.0s
Progress: 100% – Elapsed: 1.1s
Done. Total time: 1.1s
Encoding 346 subgraphs (K = 10)...
Progress: 10% – Elapsed: 0.0s
Progress: 20% – Elapsed: 0.1s
Progress: 29% – Elapsed: 0.1s
Progress: 39% – Elapsed: 0.1s
Progress: 49% – Elapsed: 0.1s
Progress: 59% – Elapsed: 0.1s
Progress: 69% – Elapsed: 0.1s
Progress: 79% – Elapsed: 0.2s
Progress: 88% – Elapsed: 0.2s
Progress: 98% – Elapsed: 0.2s
Progress: 69% – Elapsed: 0.2s
Progress: 79% – Elapsed: 0.2s
Progress: 88% – Elapsed: 0.2s
Progress: 98% – Elapsed: 0.2s
Done. Total time: 0.2s
|========================================================================================|
|  Epoch  |  Iteration  |  Time Elapsed  |  Mini-batch  |  Mini-batch  |  Base Learning  |
|         |             |   (hh:mm:ss)   |   Accuracy   |     Loss     |      Rate       |
|========================================================================================|
|       1 |           1 |       00:00:00 |       50.78% |       0.7220 |          0.1000 |
Done. Total time: 0.2s
|       5 |          50 |       00:00:00 |       79.69% |       0.3943 |          0.1000 |
|========================================================================================|
|  Epoch  |  Iteration  |  Time Elapsed  |  Mini-batch  |  Mini-batch  |  Base Learning  |
|         |             |   (hh:mm:ss)   |   Accuracy   |     Loss     |      Rate       |
|========================================================================================|
|       1 |           1 |       00:00:00 |       46.09% |       0.7299 |          0.1000 |
|       5 |          50 |       00:00:00 |       70.31% |       0.5411 |          0.1000 |
|      10 |         100 |       00:00:00 |       89.84% |       0.2844 |          0.1000 |
|      10 |         100 |       00:00:00 |       84.38% |       0.3338 |          0.1000 |
|      14 |         150 |       00:00:00 |       92.19% |       0.2221 |          0.0900 |
|      14 |         150 |       00:00:00 |       89.84% |       0.2234 |          0.0900 |
|      19 |         200 |       00:00:00 |       87.50% |       0.2291 |          0.0900 |
|      19 |         200 |       00:00:00 |       89.84% |       0.2626 |          0.0900 |
|      23 |         250 |       00:00:00 |       90.62% |       0.2355 |          0.0810 |
|      28 |         300 |       00:00:00 |       87.50% |       0.2489 |          0.0810 |
|      23 |         250 |       00:00:00 |       90.62% |       0.2046 |          0.0810 |
|      28 |         300 |       00:00:00 |       91.41% |       0.2130 |          0.0810 |
|      32 |         350 |       00:00:00 |       91.41% |       0.1850 |          0.0729 |
|      32 |         350 |       00:00:01 |       91.41% |       0.1934 |          0.0729 |
|      37 |         400 |       00:00:01 |       88.28% |       0.2136 |          0.0729 |
|      41 |         450 |       00:00:01 |       92.97% |       0.2631 |          0.0656 |
|      37 |         400 |       00:00:01 |       90.62% |       0.1918 |          0.0729 |
|      41 |         450 |       00:00:01 |       96.09% |       0.1483 |          0.0656 |
|      46 |         500 |       00:00:01 |       85.16% |       0.2370 |          0.0656 |
|      46 |         500 |       00:00:01 |       86.72% |       0.3202 |          0.0656 |
|      50 |         550 |       00:00:01 |       95.31% |       0.1535 |          0.0656 |
|      50 |         550 |       00:00:01 |       88.28% |       0.2030 |          0.0656 |
|      55 |         600 |       00:00:01 |       92.19% |       0.1576 |          0.0590 |
|      55 |         600 |       00:00:01 |       92.97% |       0.1537 |          0.0590 |
|      60 |         650 |       00:00:01 |       93.75% |       0.1364 |          0.0590 |
|      64 |         700 |       00:00:01 |       95.31% |       0.0937 |          0.0531 |
|      60 |         650 |       00:00:01 |       91.41% |       0.2018 |          0.0590 |
|      64 |         700 |       00:00:01 |       95.31% |       0.1233 |          0.0531 |
|      69 |         750 |       00:00:01 |       96.09% |       0.1029 |          0.0531 |
|      69 |         750 |       00:00:02 |       95.31% |       0.0949 |          0.0531 |
|      73 |         800 |       00:00:02 |       92.97% |       0.1347 |          0.0478 |
|      78 |         850 |       00:00:02 |       92.97% |       0.1334 |          0.0478 |
|      73 |         800 |       00:00:02 |       95.31% |       0.0917 |          0.0478 |
|      82 |         900 |       00:00:02 |       95.31% |       0.1071 |          0.0430 |
|      78 |         850 |       00:00:02 |       96.88% |       0.0857 |          0.0478 |
|      82 |         900 |       00:00:02 |       95.31% |       0.0989 |          0.0430 |
|      87 |         950 |       00:00:02 |       95.31% |       0.1220 |          0.0430 |
|      91 |        1000 |       00:00:02 |       96.09% |       0.0808 |          0.0387 |
|      87 |         950 |       00:00:02 |       90.62% |       0.1315 |          0.0430 |
|      96 |        1050 |       00:00:02 |       95.31% |       0.1036 |          0.0387 |
|      91 |        1000 |       00:00:02 |       97.66% |       0.0502 |          0.0387 |
|      96 |        1050 |       00:00:02 |       92.97% |       0.1141 |          0.0387 |
|     100 |        1100 |       00:00:02 |       92.97% |       0.1205 |          0.0387 |
|     105 |        1150 |       00:00:02 |       92.97% |       0.1170 |          0.0349 |
|     100 |        1100 |       00:00:02 |       97.66% |       0.0642 |          0.0387 |
|     110 |        1200 |       00:00:03 |       95.31% |       0.0914 |          0.0349 |
|     105 |        1150 |       00:00:02 |       94.53% |       0.1037 |          0.0349 |
|     110 |        1200 |       00:00:02 |       93.75% |       0.1202 |          0.0349 |
|     114 |        1250 |       00:00:03 |       97.66% |       0.0666 |          0.0314 |
|     114 |        1250 |       00:00:03 |       96.09% |       0.0723 |          0.0314 |
|     119 |        1300 |       00:00:03 |       96.09% |       0.0671 |          0.0314 |
|     119 |        1300 |       00:00:03 |       96.09% |       0.0752 |          0.0314 |
|     123 |        1350 |       00:00:03 |       95.31% |       0.1110 |          0.0282 |
|     123 |        1350 |       00:00:03 |       96.09% |       0.0685 |          0.0282 |
|     128 |        1400 |       00:00:03 |       93.75% |       0.1089 |          0.0282 |
|     128 |        1400 |       00:00:03 |       96.88% |       0.0668 |          0.0282 |
|     132 |        1450 |       00:00:03 |       94.53% |       0.0925 |          0.0254 |
|     132 |        1450 |       00:00:03 |       96.09% |       0.0858 |          0.0254 |
|     137 |        1500 |       00:00:03 |       95.31% |       0.1115 |          0.0254 |
|     141 |        1550 |       00:00:03 |       96.09% |       0.0827 |          0.0229 |
|     146 |        1600 |       00:00:04 |       96.09% |       0.0937 |          0.0229 |
|     150 |        1650 |       00:00:04 |       92.97% |       0.1108 |          0.0229 |
|     137 |        1500 |       00:00:03 |       94.53% |       0.1006 |          0.0254 |
|     141 |        1550 |       00:00:03 |       97.66% |       0.0389 |          0.0229 |
|     146 |        1600 |       00:00:03 |       93.75% |       0.1004 |          0.0229 |
|     150 |        1650 |       00:00:04 |       98.44% |       0.0637 |          0.0229 |
|     155 |        1700 |       00:00:04 |       94.53% |       0.1077 |          0.0206 |
|     155 |        1700 |       00:00:04 |       92.97% |       0.1020 |          0.0206 |
|     160 |        1750 |       00:00:04 |       96.88% |       0.0815 |          0.0206 |
|     164 |        1800 |       00:00:04 |       97.66% |       0.0605 |          0.0185 |
|     160 |        1750 |       00:00:04 |       94.53% |       0.1047 |          0.0206 |
|     164 |        1800 |       00:00:04 |       96.09% |       0.0667 |          0.0185 |
|     169 |        1850 |       00:00:04 |       95.31% |       0.0733 |          0.0185 |
|     169 |        1850 |       00:00:04 |       96.88% |       0.0557 |          0.0185 |
|     173 |        1900 |       00:00:04 |       96.09% |       0.0651 |          0.0167 |
|     173 |        1900 |       00:00:05 |       97.66% |       0.0768 |          0.0167 |
|     178 |        1950 |       00:00:05 |       92.97% |       0.1047 |          0.0167 |
|     178 |        1950 |       00:00:05 |       96.88% |       0.0640 |          0.0167 |
|     182 |        2000 |       00:00:05 |       96.88% |       0.0797 |          0.0150 |
|     187 |        2050 |       00:00:05 |       95.31% |       0.1047 |          0.0150 |
|     182 |        2000 |       00:00:05 |       95.31% |       0.0894 |          0.0150 |
|     187 |        2050 |       00:00:05 |       94.53% |       0.1007 |          0.0150 |
|     191 |        2100 |       00:00:05 |       97.66% |       0.0380 |          0.0135 |
|     191 |        2100 |       00:00:05 |       96.09% |       0.0762 |          0.0135 |
|     196 |        2150 |       00:00:05 |       96.09% |       0.0895 |          0.0135 |
|     196 |        2150 |       00:00:05 |       94.53% |       0.0943 |          0.0135 |
|     200 |        2200 |       00:00:06 |       92.97% |       0.1114 |          0.0135 |
|========================================================================================|
Training finished: Max epochs completed.
Best Threshold: 0.30, Precision: 0.8478, Recall: 0.9017, F1-Score: 0.8739
AUC: 0.9386
|     200 |        2200 |       00:00:06 |       97.66% |       0.0601 |          0.0135 |
|========================================================================================|
Training finished: Max epochs completed.
Best Threshold: 0.15, Precision: 0.8138, Recall: 0.8844, F1-Score: 0.8476
AUC: 0.9151
