Processing dataset: Indian Lake_tax_mass
    "Processing with K …"    "10"    " using strategy: "    "random"

    "Experiment "    "1"    " (strategy: "    "random"    ") — Running WLNM..."

[sample_neg] Final link counts:
    Train Positive: 80
    Train Negative: 160
    Test  Positive: 20
    Test  Negative: 40
[WLNM] Skipping DegreeCompute: non-degree-based strategy selected.
Encoding 240 subgraphs (K = 10)...
Progress: 10% – Elapsed: 0.0s
Encoding link 24 of 240: (3,10)
Progress: 20% – Elapsed: 0.0s
Encoding link 48 of 240: (27,14)
Progress: 30% – Elapsed: 0.0s
Encoding link 72 of 240: (35,26)
Progress: 40% – Elapsed: 0.0s
Encoding link 96 of 240: (7,31)
Progress: 50% – Elapsed: 0.0s
Encoding link 120 of 240: (33,31)
Progress: 60% – Elapsed: 0.0s
Encoding link 144 of 240: (31,11)
Progress: 70% – Elapsed: 0.1s
Encoding link 168 of 240: (18,33)
Progress: 80% – Elapsed: 0.1s
Encoding link 192 of 240: (17,29)
Progress: 90% – Elapsed: 0.1s
Encoding link 216 of 240: (24,7)
Progress: 100% – Elapsed: 0.1s
Encoding link 240 of 240: (21,28)
Done. Total time: 0.1s
Encoding 60 subgraphs (K = 10)...
Progress: 10% – Elapsed: 0.0s
Encoding link 6 of 60: (7,10)
Progress: 20% – Elapsed: 0.0s
Encoding link 12 of 60: (12,14)
Progress: 30% – Elapsed: 0.0s
Encoding link 18 of 60: (35,19)
Progress: 40% – Elapsed: 0.0s
Encoding link 24 of 60: (8,29)
Progress: 50% – Elapsed: 0.0s
Encoding link 30 of 60: (17,11)
Progress: 60% – Elapsed: 0.0s
Encoding link 36 of 60: (18,27)
Progress: 70% – Elapsed: 0.0s
Encoding link 42 of 60: (22,7)
Progress: 80% – Elapsed: 0.0s
Encoding link 48 of 60: (17,21)
Progress: 90% – Elapsed: 0.0s
Encoding link 54 of 60: (21,22)
Progress: 100% – Elapsed: 0.0s
Encoding link 60 of 60: (22,32)
Done. Total time: 0.0s
|========================================================================================|
|  Epoch  |  Iteration  |  Time Elapsed  |  Mini-batch  |  Mini-batch  |  Base Learning  |
|         |             |   (hh:mm:ss)   |   Accuracy   |     Loss     |      Rate       |
|========================================================================================|
|       1 |           1 |       00:00:00 |       14.84% |       0.7272 |          0.1000 |
|      50 |          50 |       00:00:00 |       98.44% |       0.0605 |          0.0656 |
|     100 |         100 |       00:00:00 |       98.44% |       0.0590 |          0.0387 |
|     150 |         150 |       00:00:00 |       98.44% |       0.0590 |          0.0229 |
|     200 |         200 |       00:00:00 |       98.44% |       0.0590 |          0.0135 |
|========================================================================================|
Training finished: Max epochs completed.
Best Threshold: 0.10, Precision: 0.9091, Recall: 1.0000, F1-Score: 0.9524
AUC: 0.9800
    "Experiment "    "2"    " (strategy: "    "random"    ") — Running WLNM..."

[sample_neg] Final link counts:
    Train Positive: 80
    Train Negative: 160
    Test  Positive: 20
    Test  Negative: 40
[WLNM] Skipping DegreeCompute: non-degree-based strategy selected.
Encoding 240 subgraphs (K = 10)...
Progress: 10% – Elapsed: 0.0s
Encoding link 24 of 240: (3,10)
Progress: 20% – Elapsed: 0.0s
Encoding link 48 of 240: (27,14)
Progress: 30% – Elapsed: 0.0s
Encoding link 72 of 240: (35,26)
Progress: 40% – Elapsed: 0.0s
Encoding link 96 of 240: (12,21)
Progress: 50% – Elapsed: 0.0s
Encoding link 120 of 240: (21,28)
Progress: 60% – Elapsed: 0.0s
Encoding link 144 of 240: (11,33)
Progress: 70% – Elapsed: 0.0s
Encoding link 168 of 240: (22,17)
Progress: 80% – Elapsed: 0.1s
Encoding link 192 of 240: (33,11)
Progress: 90% – Elapsed: 0.1s
Encoding link 216 of 240: (28,4)
Progress: 100% – Elapsed: 0.1s
Encoding link 240 of 240: (17,21)
Done. Total time: 0.1s
Encoding 60 subgraphs (K = 10)...
Progress: 10% – Elapsed: 0.0s
Encoding link 6 of 60: (7,10)
Progress: 20% – Elapsed: 0.0s
Encoding link 12 of 60: (12,14)
Progress: 30% – Elapsed: 0.0s
Encoding link 18 of 60: (35,19)
Progress: 40% – Elapsed: 0.0s
Encoding link 24 of 60: (1,23)
Progress: 50% – Elapsed: 0.0s
Encoding link 30 of 60: (7,32)
Progress: 60% – Elapsed: 0.0s
Encoding link 36 of 60: (33,23)
Progress: 70% – Elapsed: 0.0s
Encoding link 42 of 60: (31,7)
Progress: 80% – Elapsed: 0.0s
Encoding link 48 of 60: (1,33)
Progress: 90% – Elapsed: 0.0s
Encoding link 54 of 60: (4,18)
Progress: 100% – Elapsed: 0.0s
Encoding link 60 of 60: (18,17)
Done. Total time: 0.0s
|========================================================================================|
|  Epoch  |  Iteration  |  Time Elapsed  |  Mini-batch  |  Mini-batch  |  Base Learning  |
|         |             |   (hh:mm:ss)   |   Accuracy   |     Loss     |      Rate       |
|========================================================================================|
|       1 |           1 |       00:00:00 |       33.59% |       0.7175 |          0.1000 |
|      50 |          50 |       00:00:00 |       98.44% |       0.0579 |          0.0656 |
|     100 |         100 |       00:00:00 |       98.44% |       0.0574 |          0.0387 |
|     150 |         150 |       00:00:00 |       98.44% |       0.0574 |          0.0229 |
|     200 |         200 |       00:00:00 |       98.44% |       0.0574 |          0.0135 |
|========================================================================================|
Training finished: Max epochs completed.
Best Threshold: 0.10, Precision: 1.0000, Recall: 1.0000, F1-Score: 1.0000
AUC: 1.0000
    "Experiment "    "3"    " (strategy: "    "random"    ") — Running WLNM..."

[sample_neg] Final link counts:
    Train Positive: 80
    Train Negative: 160
    Test  Positive: 20
    Test  Negative: 40
[WLNM] Skipping DegreeCompute: non-degree-based strategy selected.
Encoding 240 subgraphs (K = 10)...
Progress: 10% – Elapsed: 0.0s
Encoding link 24 of 240: (3,10)
Progress: 20% – Elapsed: 0.0s
Encoding link 48 of 240: (27,14)
Progress: 30% – Elapsed: 0.0s
Encoding link 72 of 240: (35,26)
Progress: 40% – Elapsed: 0.0s
Encoding link 96 of 240: (24,4)
Progress: 50% – Elapsed: 0.0s
Encoding link 120 of 240: (31,4)
Progress: 60% – Elapsed: 0.0s
Encoding link 144 of 240: (24,23)
Progress: 70% – Elapsed: 0.0s
Encoding link 168 of 240: (8,21)
Progress: 80% – Elapsed: 0.1s
Encoding link 192 of 240: (22,7)
Progress: 90% – Elapsed: 0.1s
Encoding link 216 of 240: (29,8)
Progress: 100% – Elapsed: 0.1s
Encoding link 240 of 240: (31,29)
Done. Total time: 0.1s
Encoding 60 subgraphs (K = 10)...
Progress: 10% – Elapsed: 0.0s
Encoding link 6 of 60: (7,10)
Progress: 20% – Elapsed: 0.0s
Encoding link 12 of 60: (12,14)
Progress: 30% – Elapsed: 0.0s
Encoding link 18 of 60: (35,19)
Progress: 40% – Elapsed: 0.0s
Encoding link 24 of 60: (3,8)
Progress: 50% – Elapsed: 0.0s
Encoding link 30 of 60: (1,7)
Progress: 60% – Elapsed: 0.0s
Encoding link 36 of 60: (1,17)
Progress: 70% – Elapsed: 0.0s
Encoding link 42 of 60: (23,3)
Progress: 80% – Elapsed: 0.0s
Encoding link 48 of 60: (22,1)
Progress: 90% – Elapsed: 0.0s
Encoding link 54 of 60: (22,27)
Progress: 100% – Elapsed: 0.0s
Encoding link 60 of 60: (21,31)
Done. Total time: 0.0s
|========================================================================================|
|  Epoch  |  Iteration  |  Time Elapsed  |  Mini-batch  |  Mini-batch  |  Base Learning  |
|         |             |   (hh:mm:ss)   |   Accuracy   |     Loss     |      Rate       |
|========================================================================================|
|       1 |           1 |       00:00:00 |       75.00% |       0.6559 |          0.1000 |
|      50 |          50 |       00:00:00 |       99.22% |       0.0387 |          0.0656 |
|     100 |         100 |       00:00:00 |       99.22% |       0.0328 |          0.0387 |
|     150 |         150 |       00:00:00 |       99.22% |       0.0323 |          0.0229 |
|     200 |         200 |       00:00:00 |       99.22% |       0.0322 |          0.0135 |
|========================================================================================|
Training finished: Max epochs completed.
Best Threshold: 0.10, Precision: 0.9091, Recall: 1.0000, F1-Score: 0.9524
AUC: 0.9800
    "Experiment "    "4"    " (strategy: "    "random"    ") — Running WLNM..."

[sample_neg] Final link counts:
    Train Positive: 80
    Train Negative: 160
    Test  Positive: 20
    Test  Negative: 40
[WLNM] Skipping DegreeCompute: non-degree-based strategy selected.
Encoding 240 subgraphs (K = 10)...
Progress: 10% – Elapsed: 0.0s
Encoding link 24 of 240: (3,10)
Progress: 20% – Elapsed: 0.0s
Encoding link 48 of 240: (27,14)
Progress: 30% – Elapsed: 0.0s
Encoding link 72 of 240: (35,26)
Progress: 40% – Elapsed: 0.0s
Encoding link 96 of 240: (24,12)
Progress: 50% – Elapsed: 0.0s
Encoding link 120 of 240: (18,11)
Progress: 60% – Elapsed: 0.0s
Encoding link 144 of 240: (24,33)
Progress: 70% – Elapsed: 0.0s
Encoding link 168 of 240: (20,18)
Progress: 80% – Elapsed: 0.0s
Encoding link 192 of 240: (31,1)
Progress: 90% – Elapsed: 0.1s
Encoding link 216 of 240: (3,23)
Progress: 100% – Elapsed: 0.1s
Encoding link 240 of 240: (8,20)
Done. Total time: 0.1s
Encoding 60 subgraphs (K = 10)...
Progress: 10% – Elapsed: 0.0s
Encoding link 6 of 60: (7,10)
Progress: 20% – Elapsed: 0.0s
Encoding link 12 of 60: (12,14)
Progress: 30% – Elapsed: 0.0s
Encoding link 18 of 60: (35,19)
Progress: 40% – Elapsed: 0.0s
Encoding link 24 of 60: (27,29)
Progress: 50% – Elapsed: 0.0s
Encoding link 30 of 60: (12,22)
Progress: 60% – Elapsed: 0.0s
Encoding link 36 of 60: (28,29)
Progress: 70% – Elapsed: 0.0s
Encoding link 42 of 60: (20,33)
Progress: 80% – Elapsed: 0.0s
Encoding link 48 of 60: (23,31)
Progress: 90% – Elapsed: 0.0s
Encoding link 54 of 60: (28,4)
Progress: 100% – Elapsed: 0.0s
Encoding link 60 of 60: (1,12)
Done. Total time: 0.0s
|========================================================================================|
|  Epoch  |  Iteration  |  Time Elapsed  |  Mini-batch  |  Mini-batch  |  Base Learning  |
|         |             |   (hh:mm:ss)   |   Accuracy   |     Loss     |      Rate       |
|========================================================================================|
|       1 |           1 |       00:00:00 |       34.38% |       0.6987 |          0.1000 |
|      50 |          50 |       00:00:00 |       98.44% |       0.0696 |          0.0656 |
|     100 |         100 |       00:00:00 |       98.44% |       0.0461 |          0.0387 |
|     150 |         150 |       00:00:00 |       99.22% |       0.0381 |          0.0229 |
|     200 |         200 |       00:00:00 |       99.22% |       0.0363 |          0.0135 |
|========================================================================================|
Training finished: Max epochs completed.
Best Threshold: 0.10, Precision: 0.9524, Recall: 1.0000, F1-Score: 0.9756
AUC: 0.9900
    "Experiment "    "5"    " (strategy: "    "random"    ") — Running WLNM..."

[sample_neg] Final link counts:
    Train Positive: 80
    Train Negative: 160
    Test  Positive: 20
    Test  Negative: 40
[WLNM] Skipping DegreeCompute: non-degree-based strategy selected.
Encoding 240 subgraphs (K = 10)...
Progress: 10% – Elapsed: 0.0s
Encoding link 24 of 240: (3,10)
Progress: 20% – Elapsed: 0.0s
Encoding link 48 of 240: (27,14)
Progress: 30% – Elapsed: 0.0s
Encoding link 72 of 240: (35,26)
Progress: 40% – Elapsed: 0.0s
Encoding link 96 of 240: (32,17)
Progress: 50% – Elapsed: 0.0s
Encoding link 120 of 240: (3,23)
Progress: 60% – Elapsed: 0.0s
Encoding link 144 of 240: (17,20)
Progress: 70% – Elapsed: 0.0s
Encoding link 168 of 240: (33,31)
Progress: 80% – Elapsed: 0.0s
Encoding link 192 of 240: (3,18)
Progress: 90% – Elapsed: 0.0s
Encoding link 216 of 240: (33,12)
Progress: 100% – Elapsed: 0.1s
Encoding link 240 of 240: (18,33)
Done. Total time: 0.1s
Encoding 60 subgraphs (K = 10)...
Progress: 10% – Elapsed: 0.0s
Encoding link 6 of 60: (7,10)
Progress: 20% – Elapsed: 0.0s
Encoding link 12 of 60: (12,14)
Progress: 30% – Elapsed: 0.0s
Encoding link 18 of 60: (35,19)
Progress: 40% – Elapsed: 0.0s
Encoding link 24 of 60: (11,32)
Progress: 50% – Elapsed: 0.0s
Encoding link 30 of 60: (22,29)
Progress: 60% – Elapsed: 0.0s
Encoding link 36 of 60: (7,22)
Progress: 70% – Elapsed: 0.0s
Encoding link 42 of 60: (3,21)
Progress: 80% – Elapsed: 0.0s
Encoding link 48 of 60: (31,21)
Progress: 90% – Elapsed: 0.0s
Encoding link 54 of 60: (32,23)
Progress: 100% – Elapsed: 0.0s
Encoding link 60 of 60: (21,28)
Done. Total time: 0.0s
|========================================================================================|
|  Epoch  |  Iteration  |  Time Elapsed  |  Mini-batch  |  Mini-batch  |  Base Learning  |
|         |             |   (hh:mm:ss)   |   Accuracy   |     Loss     |      Rate       |
|========================================================================================|
|       1 |           1 |       00:00:00 |       36.72% |       0.7153 |          0.1000 |
|      50 |          50 |       00:00:00 |       98.44% |       0.0478 |          0.0656 |
|     100 |         100 |       00:00:00 |       99.22% |       0.0372 |          0.0387 |
|     150 |         150 |       00:00:00 |       99.22% |       0.0367 |          0.0229 |
|     200 |         200 |       00:00:00 |       99.22% |       0.0365 |          0.0135 |
|========================================================================================|
Training finished: Max epochs completed.
Best Threshold: 0.10, Precision: 0.9524, Recall: 1.0000, F1-Score: 0.9756
AUC: 0.9900
    "Experiment "    "6"    " (strategy: "    "random"    ") — Running WLNM..."

[sample_neg] Final link counts:
    Train Positive: 80
    Train Negative: 160
    Test  Positive: 20
    Test  Negative: 40
[WLNM] Skipping DegreeCompute: non-degree-based strategy selected.
Encoding 240 subgraphs (K = 10)...
Progress: 10% – Elapsed: 0.0s
Encoding link 24 of 240: (3,10)
Progress: 20% – Elapsed: 0.0s
Encoding link 48 of 240: (27,14)
Progress: 30% – Elapsed: 0.0s
Encoding link 72 of 240: (35,26)
Progress: 40% – Elapsed: 0.0s
Encoding link 96 of 240: (24,11)
Progress: 50% – Elapsed: 0.0s
Encoding link 120 of 240: (7,23)
Progress: 60% – Elapsed: 0.0s
Encoding link 144 of 240: (8,23)
Progress: 70% – Elapsed: 0.0s
Encoding link 168 of 240: (24,33)
Progress: 80% – Elapsed: 0.0s
Encoding link 192 of 240: (31,23)
Progress: 90% – Elapsed: 0.0s
Encoding link 216 of 240: (29,24)
Progress: 100% – Elapsed: 0.1s
Encoding link 240 of 240: (1,11)
Done. Total time: 0.1s
Encoding 60 subgraphs (K = 10)...
Progress: 10% – Elapsed: 0.0s
Encoding link 6 of 60: (7,10)
Progress: 20% – Elapsed: 0.0s
Encoding link 12 of 60: (12,14)
Progress: 30% – Elapsed: 0.0s
Encoding link 18 of 60: (35,19)
Progress: 40% – Elapsed: 0.0s
Encoding link 24 of 60: (7,12)
Progress: 50% – Elapsed: 0.0s
Encoding link 30 of 60: (22,33)
Progress: 60% – Elapsed: 0.0s
Encoding link 36 of 60: (4,29)
Progress: 70% – Elapsed: 0.0s
Encoding link 42 of 60: (12,20)
Progress: 80% – Elapsed: 0.0s
Encoding link 48 of 60: (3,8)
Progress: 90% – Elapsed: 0.0s
Encoding link 54 of 60: (27,1)
Progress: 100% – Elapsed: 0.0s
Encoding link 60 of 60: (23,31)
Done. Total time: 0.0s
|========================================================================================|
|  Epoch  |  Iteration  |  Time Elapsed  |  Mini-batch  |  Mini-batch  |  Base Learning  |
|         |             |   (hh:mm:ss)   |   Accuracy   |     Loss     |      Rate       |
|========================================================================================|
|       1 |           1 |       00:00:00 |       69.53% |       0.6672 |          0.1000 |
|      50 |          50 |       00:00:00 |       98.44% |       0.0581 |          0.0656 |
|     100 |         100 |       00:00:00 |       98.44% |       0.0564 |          0.0387 |
|     150 |         150 |       00:00:00 |       98.44% |       0.0563 |          0.0229 |
|     200 |         200 |       00:00:00 |       98.44% |       0.0563 |          0.0135 |
|========================================================================================|
Training finished: Max epochs completed.
Best Threshold: 0.10, Precision: 1.0000, Recall: 1.0000, F1-Score: 1.0000
AUC: 1.0000
    "Experiment "    "7"    " (strategy: "    "random"    ") — Running WLNM..."

[sample_neg] Final link counts:
    Train Positive: 80
    Train Negative: 160
    Test  Positive: 20
    Test  Negative: 40
[WLNM] Skipping DegreeCompute: non-degree-based strategy selected.
Encoding 240 subgraphs (K = 10)...
Progress: 10% – Elapsed: 0.0s
Encoding link 24 of 240: (3,10)
Progress: 20% – Elapsed: 0.0s
Encoding link 48 of 240: (27,14)
Progress: 30% – Elapsed: 0.0s
Encoding link 72 of 240: (35,26)
Progress: 40% – Elapsed: 0.0s
Encoding link 96 of 240: (33,12)
Progress: 50% – Elapsed: 0.0s
Encoding link 120 of 240: (11,8)
Progress: 60% – Elapsed: 0.0s
Encoding link 144 of 240: (33,7)
Progress: 70% – Elapsed: 0.0s
Encoding link 168 of 240: (1,32)
Progress: 80% – Elapsed: 0.0s
Encoding link 192 of 240: (7,20)
Progress: 90% – Elapsed: 0.0s
Encoding link 216 of 240: (31,1)
Progress: 100% – Elapsed: 0.1s
Encoding link 240 of 240: (27,18)
Done. Total time: 0.1s
Encoding 60 subgraphs (K = 10)...
Progress: 10% – Elapsed: 0.0s
Encoding link 6 of 60: (7,10)
Progress: 20% – Elapsed: 0.0s
Encoding link 12 of 60: (12,14)
Progress: 30% – Elapsed: 0.0s
Encoding link 18 of 60: (35,19)
Progress: 40% – Elapsed: 0.0s
Encoding link 24 of 60: (23,31)
Progress: 50% – Elapsed: 0.0s
Encoding link 30 of 60: (1,28)
Progress: 60% – Elapsed: 0.0s
Encoding link 36 of 60: (24,11)
Progress: 70% – Elapsed: 0.0s
Encoding link 42 of 60: (27,28)
Progress: 80% – Elapsed: 0.0s
Encoding link 48 of 60: (3,27)
Progress: 90% – Elapsed: 0.0s
Encoding link 54 of 60: (33,4)
Progress: 100% – Elapsed: 0.0s
Encoding link 60 of 60: (24,22)
Done. Total time: 0.0s
|========================================================================================|
|  Epoch  |  Iteration  |  Time Elapsed  |  Mini-batch  |  Mini-batch  |  Base Learning  |
|         |             |   (hh:mm:ss)   |   Accuracy   |     Loss     |      Rate       |
|========================================================================================|
|       1 |           1 |       00:00:00 |       46.09% |       0.6929 |          0.1000 |
|      50 |          50 |       00:00:00 |       95.31% |       0.1231 |          0.0656 |
|     100 |         100 |       00:00:00 |       96.88% |       0.1052 |          0.0387 |
|     150 |         150 |       00:00:00 |       96.88% |       0.1024 |          0.0229 |
|     200 |         200 |       00:00:00 |       96.88% |       0.1016 |          0.0135 |
|========================================================================================|
Training finished: Max epochs completed.
Best Threshold: 0.10, Precision: 1.0000, Recall: 1.0000, F1-Score: 1.0000
AUC: 1.0000
    "Experiment "    "8"    " (strategy: "    "random"    ") — Running WLNM..."

[sample_neg] Final link counts:
    Train Positive: 80
    Train Negative: 160
    Test  Positive: 20
    Test  Negative: 40
[WLNM] Skipping DegreeCompute: non-degree-based strategy selected.
Encoding 240 subgraphs (K = 10)...
Progress: 10% – Elapsed: 0.0s
Encoding link 24 of 240: (3,10)
Progress: 20% – Elapsed: 0.0s
Encoding link 48 of 240: (27,14)
Progress: 30% – Elapsed: 0.0s
Encoding link 72 of 240: (35,26)
Progress: 40% – Elapsed: 0.0s
Encoding link 96 of 240: (24,11)
Progress: 50% – Elapsed: 0.0s
Encoding link 120 of 240: (12,29)
Progress: 60% – Elapsed: 0.0s
Encoding link 144 of 240: (1,17)
Progress: 70% – Elapsed: 0.0s
Encoding link 168 of 240: (32,20)
Progress: 80% – Elapsed: 0.0s
Encoding link 192 of 240: (32,23)
Progress: 90% – Elapsed: 0.0s
Encoding link 216 of 240: (3,11)
Progress: 100% – Elapsed: 0.1s
Encoding link 240 of 240: (3,28)
Done. Total time: 0.1s
Encoding 60 subgraphs (K = 10)...
Progress: 10% – Elapsed: 0.0s
Encoding link 6 of 60: (7,10)
Progress: 20% – Elapsed: 0.0s
Encoding link 12 of 60: (12,14)
Progress: 30% – Elapsed: 0.0s
Encoding link 18 of 60: (35,19)
Progress: 40% – Elapsed: 0.0s
Encoding link 24 of 60: (20,7)
Progress: 50% – Elapsed: 0.0s
Encoding link 30 of 60: (20,29)
Progress: 60% – Elapsed: 0.0s
Encoding link 36 of 60: (1,29)
Progress: 70% – Elapsed: 0.0s
Encoding link 42 of 60: (23,31)
Progress: 80% – Elapsed: 0.0s
Encoding link 48 of 60: (17,22)
Progress: 90% – Elapsed: 0.0s
Encoding link 54 of 60: (31,12)
Progress: 100% – Elapsed: 0.0s
Encoding link 60 of 60: (20,32)
Done. Total time: 0.0s
|========================================================================================|
|  Epoch  |  Iteration  |  Time Elapsed  |  Mini-batch  |  Mini-batch  |  Base Learning  |
|         |             |   (hh:mm:ss)   |   Accuracy   |     Loss     |      Rate       |
|========================================================================================|
|       1 |           1 |       00:00:00 |       57.81% |       0.7065 |          0.1000 |
|      50 |          50 |       00:00:00 |       98.44% |       0.1322 |          0.0656 |
|     100 |         100 |       00:00:00 |       98.44% |       0.0588 |          0.0387 |
|     150 |         150 |       00:00:00 |       98.44% |       0.0579 |          0.0229 |
|     200 |         200 |       00:00:00 |       98.44% |       0.0576 |          0.0135 |
|========================================================================================|
Training finished: Max epochs completed.
Best Threshold: 0.10, Precision: 1.0000, Recall: 1.0000, F1-Score: 1.0000
AUC: 1.0000
    "Experiment "    "9"    " (strategy: "    "random"    ") — Running WLNM..."

[sample_neg] Final link counts:
    Train Positive: 80
    Train Negative: 160
    Test  Positive: 20
    Test  Negative: 40
[WLNM] Skipping DegreeCompute: non-degree-based strategy selected.
Encoding 240 subgraphs (K = 10)...
Progress: 10% – Elapsed: 0.0s
Encoding link 24 of 240: (3,10)
Progress: 20% – Elapsed: 0.0s
Encoding link 48 of 240: (27,14)
Progress: 30% – Elapsed: 0.0s
Encoding link 72 of 240: (35,26)
Progress: 40% – Elapsed: 0.0s
Encoding link 96 of 240: (20,4)
Progress: 50% – Elapsed: 0.0s
Encoding link 120 of 240: (18,23)
Progress: 60% – Elapsed: 0.0s
Encoding link 144 of 240: (4,24)
Progress: 70% – Elapsed: 0.0s
Encoding link 168 of 240: (1,28)
Progress: 80% – Elapsed: 0.0s
Encoding link 192 of 240: (24,32)
Progress: 90% – Elapsed: 0.1s
Encoding link 216 of 240: (29,27)
Progress: 100% – Elapsed: 0.1s
Encoding link 240 of 240: (29,1)
Done. Total time: 0.1s
Encoding 60 subgraphs (K = 10)...
Progress: 10% – Elapsed: 0.0s
Encoding link 6 of 60: (7,10)
Progress: 20% – Elapsed: 0.0s
Encoding link 12 of 60: (12,14)
Progress: 30% – Elapsed: 0.0s
Encoding link 18 of 60: (35,19)
Progress: 40% – Elapsed: 0.0s
Encoding link 24 of 60: (20,3)
Progress: 50% – Elapsed: 0.0s
Encoding link 30 of 60: (8,24)
Progress: 60% – Elapsed: 0.0s
Encoding link 36 of 60: (23,4)
Progress: 70% – Elapsed: 0.0s
Encoding link 42 of 60: (1,4)
Progress: 80% – Elapsed: 0.0s
Encoding link 48 of 60: (4,27)
Progress: 90% – Elapsed: 0.0s
Encoding link 54 of 60: (32,28)
Progress: 100% – Elapsed: 0.0s
Encoding link 60 of 60: (24,17)
Done. Total time: 0.0s
|========================================================================================|
|  Epoch  |  Iteration  |  Time Elapsed  |  Mini-batch  |  Mini-batch  |  Base Learning  |
|         |             |   (hh:mm:ss)   |   Accuracy   |     Loss     |      Rate       |
|========================================================================================|
|       1 |           1 |       00:00:00 |       39.06% |       0.6934 |          0.1000 |
|      50 |          50 |       00:00:00 |       99.22% |       0.0378 |          0.0656 |
|     100 |         100 |       00:00:00 |       99.22% |       0.0362 |          0.0387 |
|     150 |         150 |       00:00:00 |       99.22% |       0.0361 |          0.0229 |
|     200 |         200 |       00:00:00 |       99.22% |       0.0361 |          0.0135 |
|========================================================================================|
Training finished: Max epochs completed.
Best Threshold: 0.10, Precision: 0.8696, Recall: 1.0000, F1-Score: 0.9302
AUC: 0.9700
    "Experiment "    "10"    " (strategy: "    "random"    ") — Running WLNM..."

[sample_neg] Final link counts:
    Train Positive: 80
    Train Negative: 160
    Test  Positive: 20
    Test  Negative: 40
[WLNM] Skipping DegreeCompute: non-degree-based strategy selected.
Encoding 240 subgraphs (K = 10)...
Progress: 10% – Elapsed: 0.0s
Encoding link 24 of 240: (3,10)
Progress: 20% – Elapsed: 0.0s
Encoding link 48 of 240: (27,14)
Progress: 30% – Elapsed: 0.0s
Encoding link 72 of 240: (35,26)
Progress: 40% – Elapsed: 0.0s
Encoding link 96 of 240: (29,21)
Progress: 50% – Elapsed: 0.0s
Encoding link 120 of 240: (3,1)
Progress: 60% – Elapsed: 0.0s
Encoding link 144 of 240: (22,33)
Progress: 70% – Elapsed: 0.0s
Encoding link 168 of 240: (17,7)
Progress: 80% – Elapsed: 0.0s
Encoding link 192 of 240: (23,7)
Progress: 90% – Elapsed: 0.1s
Encoding link 216 of 240: (7,27)
Progress: 100% – Elapsed: 0.1s
Encoding link 240 of 240: (23,1)
Done. Total time: 0.1s
Encoding 60 subgraphs (K = 10)...
Progress: 10% – Elapsed: 0.0s
Encoding link 6 of 60: (7,10)
Progress: 20% – Elapsed: 0.0s
Encoding link 12 of 60: (12,14)
Progress: 30% – Elapsed: 0.0s
Encoding link 18 of 60: (35,19)
Progress: 40% – Elapsed: 0.0s
Encoding link 24 of 60: (24,12)
Progress: 50% – Elapsed: 0.0s
Encoding link 30 of 60: (7,33)
Progress: 60% – Elapsed: 0.0s
Encoding link 36 of 60: (22,1)
Progress: 70% – Elapsed: 0.0s
Encoding link 42 of 60: (1,20)
Progress: 80% – Elapsed: 0.0s
Encoding link 48 of 60: (23,3)
Progress: 90% – Elapsed: 0.0s
Encoding link 54 of 60: (31,18)
Progress: 100% – Elapsed: 0.0s
Encoding link 60 of 60: (31,11)
Done. Total time: 0.0s
|========================================================================================|
|  Epoch  |  Iteration  |  Time Elapsed  |  Mini-batch  |  Mini-batch  |  Base Learning  |
|         |             |   (hh:mm:ss)   |   Accuracy   |     Loss     |      Rate       |
|========================================================================================|
|       1 |           1 |       00:00:00 |       67.19% |       0.6933 |          0.1000 |
|      50 |          50 |       00:00:00 |      100.00% |       0.0049 |          0.0656 |
|     100 |         100 |       00:00:00 |      100.00% |       0.0003 |          0.0387 |
|     150 |         150 |       00:00:00 |      100.00% |       0.0003 |          0.0229 |
|     200 |         200 |       00:00:00 |      100.00% |       0.0002 |          0.0135 |
|========================================================================================|
Training finished: Max epochs completed.
Best Threshold: 0.10, Precision: 1.0000, Recall: 1.0000, F1-Score: 1.0000
AUC: 1.0000
